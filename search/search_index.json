{"config":{"lang":["it"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ML-Lab","text":"<p>Benvenuto! Questo \u00e8 il laboratorio pratico per capire davvero ML: - modelli semplici ma realistici, - soglie (non solo 0.5), - importanze delle feature, - niente leakage (pipeline + CV), - report \u201cparlanti\u201d.</p> <p>\ud83d\udc49 Codice: repo GitHub Usa il menu a sinistra per navigare. Buono studio! \ud83d\udcaa</p>"},{"location":"lessons-learned/","title":"ML \u2014 Lessons Learned (versione \u201cterra-terra\u201d, aggiornata)","text":"<p>Di seguito il riassunto che useremmo davvero in lab: spiegato semplice, con esempi concreti e zero fuffa.</p>"},{"location":"lessons-learned/#0-mini-glossario-3-parole-che-tornano-sempre","title":"0) Mini-glossario (3 parole che tornano sempre)","text":"<ul> <li>Parametri: numeri che il modello impara (es. pesi della logistica).</li> <li>Iperparametri: manopole che scegli tu (es. <code>max_depth</code> di un albero, <code>C</code> della logistica).</li> <li>Fit: l\u2019atto di far apprendere il modello dal train.</li> </ul>"},{"location":"lessons-learned/#1-cosa-stiamo-facendo","title":"1) Cosa stiamo facendo","text":"<ul> <li>ML = imparare una funzione <code>input \u2192 output</code> dai dati.   Non scrivi la formula a mano: la stima il modello minimizzando un errore.</li> <li>Metodo onesto: separa sempre i dati in train (si impara) e test (si misura).   Il test \u00e8 sacro: non lo usi per scegliere nulla.</li> </ul>"},{"location":"lessons-learned/#2-fit-buono-vs-fit-cattivo","title":"2) Fit buono vs fit cattivo","text":"<ul> <li>Overfitting: vola su train, crolla su test \u2192 ha imparato anche il rumore.   Sintomi: modello troppo complesso, regole iper-specifiche.</li> <li>Underfitting: scarso sia su train sia su test \u2192 modello troppo semplice o features povere.</li> <li>Che fare: confronta train vs test, regola la complessit\u00e0 (es. <code>max_depth</code>, <code>C</code>), aggiungi features/dati migliori.</li> </ul>"},{"location":"lessons-learned/#3-decision-tree-albero","title":"3) Decision Tree (albero)","text":"<ul> <li>Pro: regole leggibili (SE \u2026 ALLORA \u2026), spiegazioni facili.</li> <li>Manopole principali: <code>max_depth</code>, <code>min_samples_leaf</code>, <code>max_leaf_nodes</code>.</li> <li>Nota: un singolo albero \u00e8 instabile (alta varianza). La Random Forest media molti alberi e stabilizza.</li> </ul>"},{"location":"lessons-learned/#4-come-scegliere-gli-iperparametri-senza-barare","title":"4) Come scegliere gli iperparametri senza barare","text":"<ul> <li>k-fold cross-validation sul solo train (es. 5-fold).   Media e deviazione ti dicono qualit\u00e0 e incertezza.</li> <li>Regola \u201c1-SE\u201d: se pi\u00f9 settaggi sono quasi pari, scegli il pi\u00f9 semplice.</li> <li>Dataset piccolo? Fai CV ripetuta (es. 5\u00d75-fold) per avere una media pi\u00f9 stabile.</li> </ul>"},{"location":"lessons-learned/#5-classi-sbilanciate-laccuracy-mente","title":"5) Classi sbilanciate: l\u2019accuracy mente","text":"<ul> <li>Se il 90% \u00e8 classe 0, dire sempre \u201c0\u201d d\u00e0 accuracy 0.90\u2026 ma non serve a nulla.</li> <li>Guarda Precision (pochi falsi allarmi), Recall (pochi positivi persi), F1 (equilibrio tra i due).</li> <li>Confusion matrix = tavolo della verit\u00e0: TN, FP, FN, TP.</li> </ul> <p>Esempio lampo (100 casi): Predici 40 positivi \u2192 30 giusti (TP) e 10 falsi allarmi (FP). Ci sono 35 positivi reali: ne hai persi 5 (FN).</p> <ul> <li>Precision = 30/(30+10)=0.75</li> <li>Recall = 30/(30+5)=0.86</li> <li>F1 \u2248 0.80</li> </ul>"},{"location":"lessons-learned/#6-f1-e-roc-auc-in-due-righe","title":"6) F1 e ROC-AUC in due righe","text":"<ul> <li>F1: numero unico che sale solo se precision e recall sono entrambi alti.   <code>F1 = 2 \u00b7 (Prec\u00b7Rec) / (Prec + Rec)</code></li> <li>ROC-AUC: qualit\u00e0 del ranking delle probabilit\u00e0 su tutte le soglie.   0.5 = a caso, 1.0 = perfetto. Non dipende dalla soglia 0.5.</li> </ul> <p>Nota: con sbilanciamenti molto forti, guarda anche la PR-AUC (area Precision-Recall).</p>"},{"location":"lessons-learned/#7-la-soglia-05-non-e-legge","title":"7) La soglia: 0.5 non \u00e8 legge","text":"<p>Cambiare soglia scambia FP \u2194 FN. Tre modi pratici per sceglierla:</p> <ol> <li>Auto-threshold su validation (dal train): massimizza F1 (o Youden <code>TPR\u2212FPR</code>).</li> <li>A costo: scegli <code>soglia</code> che minimizza <code>c_fp\u00b7FP + c_fn\u00b7FN</code> (decidi tu quanto pesa un FN vs FP).</li> <li>Sweep tabellare: stampa soglia \u2192 (FP, FN, Precision, Recall, F1) e guardala.</li> </ol> <p>Regola pratica:</p> <ul> <li>Se perdere un positivo fa male \u2192 soglia pi\u00f9 bassa (recall su, pi\u00f9 FP).</li> <li>Se odi i falsi allarmi \u2192 soglia pi\u00f9 alta (precision su, pi\u00f9 FN).</li> </ul>"},{"location":"lessons-learned/#8-scaling-quando-si-e-quando-no","title":"8) Scaling: quando s\u00ec e quando no","text":"<ul> <li>Serve a modelli basati su distanze/geometria: Logistica, SVM, k-NN, PCA.</li> <li>Non serve in genere per Alberi/Random Forest (si basano su soglie, non su distanze).</li> </ul>"},{"location":"lessons-learned/#9-random-forest-perche-funziona-e-dove-punge","title":"9) Random Forest: perch\u00e9 funziona e dove punge","text":"<ul> <li>Pro: cattura non-linearit\u00e0 e interazioni, robusta al rumore, buona \u201cout-of-the-box\u201d.</li> <li>Contro tipico: le probabilit\u00e0 possono essere poco calibrate \u2192 0.5 spesso non \u00e8 la soglia giusta.</li> <li> <p>Rimedi:</p> </li> <li> <p>scegli bene la soglia (auto/costo/sweep),</p> </li> <li>valuta <code>class_weight='balanced'</code> se la positiva \u00e8 rara,</li> <li>calibra le probabilit\u00e0 (sigmoid/Platt o isotonic) se ti servono score affidabili.</li> </ul>"},{"location":"lessons-learned/#10-importanza-delle-feature-evitare-abbagli","title":"10) \u201cImportanza\u201d delle feature: evitare abbagli","text":"<ul> <li>Impurity importance (<code>feature_importances_</code>): rapida, ma favorisce variabili con molte soglie e soffre con feature molto correlate (le \u201csorelle\u201d si dividono il merito).</li> <li>Permutation importance (su TEST e con una metrica, es. ROC-AUC): misuri quanto peggiora il modello quando rompi una feature \u2192 spesso pi\u00f9 onesta.</li> <li>Ablation test: rimuovi la top-1/top-k e rimisura. Se l\u2019AUC non scende, c\u2019\u00e8 ridondanza (pi\u00f9 feature dicono la stessa cosa).</li> </ul>"},{"location":"lessons-learned/#11-leakage-barare-senza-volerlo","title":"11) Leakage: barare senza volerlo","text":"<ul> <li>Cos\u2019\u00e8: usare info del test (o del fold di validazione) per calcolare imputazioni, scaling, encoding, selezione feature\u2026   Risultato: metriche gonfiate.</li> <li>Regola d\u2019oro: tutte le trasformazioni si fittano solo sul train.</li> <li>Strumenti che ti proteggono: Pipeline (e ColumnTransformer per numeriche+categoriche) + GridSearchCV sulla pipeline.</li> <li>Spia rossa: CV troppo bella dopo aver preprocessato prima della CV.</li> </ul>"},{"location":"lessons-learned/#12-workflow-che-non-ti-tradisce","title":"12) Workflow che non ti tradisce","text":"<ol> <li>Split train/test (il test in cassaforte).</li> <li>Sul train: costruisci Pipeline (preprocess + modello).</li> <li>CV (anche ripetuta) per tuning iperparametri (regola 1-SE se pari).</li> <li>Soglia: scegli su validation/OOF o per costo.</li> <li>Fit finale sul train completo con pipeline + iperparametri + soglia scelti.</li> <li>Test una sola volta: F1, ROC-AUC, confusion; se serve, calibrazione (Brier, reliability bins).</li> <li>Spiega: importanze (impurity + permutation) e, se serve, ablation.</li> </ol>"},{"location":"lessons-learned/#13-regole-veloci-da-campo","title":"13) Regole veloci da campo","text":"<ul> <li>Mai usare il test per scegliere iperparametri o soglia.</li> <li>Dataset piccolo \u2192 CV ripetuta + regola 1-SE.</li> <li>Sbilanciamento: valuta F1 e/o Recall (se i FN costano), usa ROC-AUC per confrontare modelli e PR-AUC se lo sbilanciamento \u00e8 estremo.</li> <li>RF: non fissarti su 0.5; scegli soglia e valuta calibrazione.</li> <li>Logistica: con scaling, spesso imbattibile su confini quasi lineari; probabilit\u00e0 molto affidabili.</li> </ul>"},{"location":"lessons-learned/#14-debug-veloce-checklist","title":"14) Debug veloce (checklist)","text":"<ol> <li>Baseline: quanto fa un dummy che predice la classe pi\u00f9 frequente?</li> <li>Leakage: sto facendo imputazione/scaling fuori pipeline?</li> <li>Metriche giuste: sto guardando F1/Recall oltre all\u2019accuracy?</li> <li>Soglia: ho provato auto-threshold o a costo?</li> <li>Varianza: risultati stabili cambiando seed? (se no, CV ripetuta)</li> <li>Ridondanza: feature molto correlate? Usa permutation + ablation.</li> </ol>"},{"location":"quiz/","title":"ML-Lab \u2014 Quiz Q&amp;A (versione chiara)","text":"<p>Studio autonomo \u2014 domande &amp; risposte semplici ma dettagliate.</p> <p>Consiglio: prova prima a rispondere da solo, poi confronta con la soluzione. Ogni risposta include il \u201cperch\u00e9\u201d e spesso un mini-esempio pratico.</p>"},{"location":"quiz/#1-che-cose-in-una-frase-il-machine-learning","title":"1) Che cos\u2019\u00e8, in una frase, il Machine Learning?","text":"<p>Risposta (umana): Costruire un programma/funzione che impara una determinata regola dai dati: gli mostri esempi di input e il risultato giusto, e lui regola da solo le sue manopole per sbagliare il meno possibile sugli esempi, puntando ad andare bene anche su dati nuovi. - Parametri = manopole del modello (numeri). - Algoritmo di apprendimento = come giriamo le manopole per ridurre l\u2019errore. - Generalizzazione = andare bene su test nuovi, su dati mai visti.</p>"},{"location":"quiz/#2-perche-il-test-e-sacro","title":"2) Perch\u00e9 il \u201ctest \u00e8 sacro\u201d?","text":"<p>Risposta: Serve a misurare la generalizzazione vera. Se lo usi per scegliere iperparametri (o le soglie), inquini la misura (stai \u201cimparando\u201d anche dal test ed \u00e8 scorretto). Regola: tutte le decisioni (tuning, soglia, trasformazioni) vanno prese solo col train (CV/validation). Il test si usa una volta alla fine.</p>"},{"location":"quiz/#3-overfitting-vs-underfitting-differenza-pratica-e-rimedi","title":"3) Overfitting vs Underfitting: differenza pratica e rimedi","text":"<p>Overfit: ottimo su train, peggio su test \u2192 troppo complesso / ha \u201cpappagallato\u201d. Underfit: scarso su train e test \u2192 troppo semplice / info insufficienti. Rimedi: riduci o aumenti la complessit\u00e0 (es. <code>max_depth</code>, <code>C</code>), aggiungi dati/feature, regolarizzazione, semplifica il modello.</p>"},{"location":"quiz/#4-decision-tree-perche-e-spiegabile-e-come-ne-controlli-la-complessita","title":"4) Decision Tree: perch\u00e9 \u00e8 spiegabile e come ne controlli la complessit\u00e0?","text":"<p>Risposta: L\u2019albero produce regole SE \u2026 ALLORA \u2026 leggibili (threshold sulle feature). Controllo: <code>max_depth</code>, <code>min_samples_leaf</code>, <code>max_leaf_nodes</code>. Nota: un albero singolo ha alta varianza \u2192 piccole variazioni nei dati cambiano molto la struttura.</p>"},{"location":"quiz/#5-perche-una-random-forest-riduce-la-varianza-dellalbero-singolo","title":"5) Perch\u00e9 una Random Forest riduce la varianza dell\u2019albero singolo?","text":"<p>Risposta: Fa la media di molti alberi addestrati su campioni diversi e sottoinsiemi di feature (bagging + casualit\u00e0). La media stabilizza e riduce errori \u201ccapricciosi\u201d.</p>"},{"location":"quiz/#6-cose-un-iperparametro-esempi","title":"6) Cos\u2019\u00e8 un iperparametro? Esempi","text":"<p>Risposta: \u00c8 un controllo del modello che non viene imparato dai dati ma scelto dall\u2019utente con CV. Esempi: <code>max_depth</code> (Tree/RF), <code>n_estimators</code> (RF), <code>C</code> (Logit/SVM), <code>max_features</code> (RF), <code>learning_rate</code> (boosting).</p>"},{"location":"quiz/#7-k-fold-cross-validation-cose-e-perche-sul-solo-train","title":"7) k-fold Cross-Validation: cos\u2019\u00e8 e perch\u00e9 sul solo train?","text":"<p>Risposta: Dividi il train in k parti; ripeti k volte: alleni su k-1 parti, valuti sulla parte rimasta. Media e deviazione standard ti danno una stima stabile. Si fa solo sul train per non \u201cguardare\u201d il test.</p>"},{"location":"quiz/#8-regola-1-se-quando-e-perche","title":"8) Regola 1-SE: quando e perch\u00e9?","text":"<p>Risposta: Se pi\u00f9 modelli sono statisticamente equivalenti (entro 1 deviazione standard dalla migliore metrica), scegli la versione pi\u00f9 semplice. \u00c8 pi\u00f9 robusta su dati nuovi.</p>"},{"location":"quiz/#9-perche-laccuracy-puo-mentire-con-classi-sbilanciate","title":"9) Perch\u00e9 l\u2019accuracy pu\u00f2 mentire con classi sbilanciate?","text":"<p>Risposta: Se i positivi sono rari, dire \u201csempre negativo\u201d pu\u00f2 dare alta accuracy ma recall=0. Soluzione: guarda Precision, Recall, F1 e la Confusion matrix.</p>"},{"location":"quiz/#10-precision-recall-f1-definizioni-pratiche","title":"10) Precision, Recall, F1: definizioni pratiche","text":"<p>Precision = tra gli allarmi che hai dato, quanti erano giusti? = <code>TP/(TP+FP)</code> Recall = tra i positivi veri, quanti non hai perso? = <code>TP/(TP+FN)</code> F1 = media armonica di Precision e Recall \u2192 sale solo se entrambi sono alti. Quando usarle: problemi sbilanciati o quando i costi di FP/FN sono diversi.</p>"},{"location":"quiz/#11-roc-auc-cosa-misura-e-perche-non-dipende-da-05","title":"11) ROC-AUC: cosa misura e perch\u00e9 non dipende da 0.5","text":"<p>Risposta: Valuta la qualit\u00e0 del ranking delle probabilit\u00e0 su tutte le soglie. Interpretazione: probabilit\u00e0 che un positivo abbia score &gt; di un negativo. 0.5 \u2248 a caso, 1.0 perfetto. Non usa la soglia fissa 0.5.</p>"},{"location":"quiz/#12-come-si-sceglie-la-soglia-tre-strade","title":"12) Come si sceglie la soglia? Tre strade","text":"<p>1) Auto-threshold su validation/OOF per massimizzare F1 o Youden = TPR\u2212FPR. 2) A costo: minimizza <code>c_fp\u00b7FP + c_fn\u00b7FN</code> (decidi tu quanto pesa perdere un positivo). 3) Sweep: stampi la tabella soglia \u2192 (FP, FN, Precision, Recall, F1) e scegli consapevolmente. Regola pratica: se FN \u00e8 gravissimo, abbassa la soglia.</p>"},{"location":"quiz/#13-cosa-sono-youden-e-oof","title":"13) Cosa sono Youden e OOF?","text":"<p>Youden J: <code>TPR \u2212 FPR</code> \u2192 cerca la soglia pi\u00f9 distante dalla diagonale della ROC. OOF (Out-Of-Fold): predizioni di CV sul train, fatte da modelli che non hanno visto quel campione. Servono per scelte oneste (soglia, calibrazione) senza toccare il test.</p>"},{"location":"quiz/#14-quando-serve-lo-scaling","title":"14) Quando serve lo scaling?","text":"<p>Risposta: Serve per modelli che usano distanze o geometria (Logit, SVM, k-NN, PCA). In genere non serve per Tree/Random Forest (lavorano per soglie, non distanze).</p>"},{"location":"quiz/#15-logistic-regression-procontro-e-iper-chiave","title":"15) Logistic Regression: pro/contro e iper chiave","text":"<p>Pro: semplice, veloce, probabilit\u00e0 ben calibrate, ottima su confini quasi lineari. Contro: fatica con forte non-linearit\u00e0 se non aggiungi feature/interazioni. Iper: <code>C</code> (meno regolarizzazione se alto) \u2192 occhio all\u2019overfit; scalare le feature \u00e8 essenziale.</p>"},{"location":"quiz/#16-random-forest-procontro-e-pomelli","title":"16) Random Forest: pro/contro e pomelli","text":"<p>Pro: coglie non-linearit\u00e0 e interazioni, robusta, pochi settaggi critici. Contro: probabilit\u00e0 spesso non calibrate; spiegabilit\u00e0 globale limitata. Pomelli: <code>n_estimators</code>, <code>max_depth</code>, <code>max_features</code>, <code>class_weight</code> per sbilanciamento.</p>"},{"location":"quiz/#17-perche-calibrare-le-probabilita-di-una-rf-come","title":"17) Perch\u00e9 calibrare le probabilit\u00e0 di una RF? Come?","text":"<p>Risposta: Perch\u00e9 gli score possono essere troppo ottimisti/piatti. Con <code>CalibratedClassifierCV</code>: - sigmoid (Platt): semplice, pochi dati. - isotonic: pi\u00f9 flessibile, richiede pi\u00f9 dati. Trucco pratico: per leggere <code>feature_importances_</code> quando usi la calibrazione, rifitta un clone della RF non calibrata sul train.</p>"},{"location":"quiz/#18-importanza-delle-feature-impurity-vs-permutation","title":"18) Importanza delle feature: \u201cimpurity\u201d vs \u201cpermutation\u201d","text":"<p>Impurity (feature_importances_): veloce, ma favorisce feature con tante soglie; con feature correlate si divide il merito. Permutation (su TEST): misura il calo reale di metrica quando \u201crompi\u201d la feature \u2192 spesso pi\u00f9 onesta. Uso consigliato: guarda entrambe e conferma con ablation.</p>"},{"location":"quiz/#19-effetto-delle-feature-molto-correlate","title":"19) Effetto delle feature molto correlate","text":"<p>Risposta: Se due feature dicono la stessa cosa, l\u2019importanza si spalma su entrambe: nessuna sembra \u201cdominare\u201d da sola. Cosa fare: controlla correlazioni (|\u03c1|\u22650.9), prova ablation, valuta grouping/RIDUZIONE (PCA o scelta di una sola).</p>"},{"location":"quiz/#20-cose-lablation-test","title":"20) Cos\u2019\u00e8 l\u2019ablation test?","text":"<p>Risposta: Togli le top-1/top-k feature e misuri quanto scende la metrica (es. AUC). Lettura: se non scende, il segnale era ridondante; se crolla, quelle feature sono cruciali.</p>"},{"location":"quiz/#21-cose-il-data-leakage-esempio-e-prevenzione","title":"21) Cos\u2019\u00e8 il data leakage? Esempio e prevenzione","text":"<p>Leakage: usi info del futuro/test nell\u2019addestramento. Esempio: scalare o imputare sull\u2019intero dataset prima dello split/CV. Prevenzione: tutte le trasformazioni in Pipeline/ColumnTransformer, cos\u00ec ogni fold fitta solo sul proprio train.</p>"},{"location":"quiz/#22-perche-pipeline-gridsearchcv-previene-il-leakage","title":"22) Perch\u00e9 Pipeline + GridSearchCV previene il leakage?","text":"<p>Risposta: Perch\u00e9 imputazione/scaling/encoding e modello sono insieme nella pipeline e vengono fittati dentro la CV solo sui dati di train del fold. Il fold di validazione (e il test finale) restano puliti.</p>"},{"location":"quiz/#23-predizioni-oof-perche-usarle-per-la-soglia","title":"23) Predizioni OOF: perch\u00e9 usarle per la soglia?","text":"<p>Risposta: Sono predizioni su campioni mai visti dal rispettivo modello di fold \u2192 sono oneste come un mini-test interno. Permettono di scegliere soglie/calibrazioni senza toccare il test.</p>"},{"location":"quiz/#24-refit-su-f1-cosa-vuol-dire-e-come-valuti-il-test","title":"24) \u201cRefit su F1\u201d: cosa vuol dire e come valuti il test?","text":"<p>Risposta: In <code>GridSearchCV(refit=\"f1\")</code>, dopo la CV si ri-addestra il miglior modello (per F1) su tutto il train. Poi si valuta una sola volta sul test (niente ulteriori scelte sul test).</p>"},{"location":"quiz/#25-confusion-matrix-lettura-e-collegamento-ai-costi","title":"25) Confusion matrix: lettura e collegamento ai costi","text":"<p>TN: negativi corretti; FP: falsi allarmi; FN: positivi persi; TP: positivi presi. Costo totale: <code>c_fp\u00b7FP + c_fn\u00b7FN</code>. Soglia a costo: scegli la soglia che minimizza questo valore, in base al tuo scenario.</p>"},{"location":"quiz/#26-logit-o-random-forest-quando-scegliere-cosa","title":"26) Logit o Random Forest? Quando scegliere cosa","text":"<p>Logit: confini lineari, bisogno di probabilit\u00e0 affidabili, spiegabilit\u00e0 dei coefficienti. RF: dati non-lineari, interazioni, \u201cout-of-the-box\u201d forte; poi regola soglia e, se servono probabilit\u00e0 affidabili, calibra.</p>"},{"location":"quiz/#27-class_weightbalanced-cosa-fa-e-quando-usarlo","title":"27) <code>class_weight='balanced'</code>: cosa fa e quando usarlo?","text":"<p>Risposta: Pesa gli errori della classe minoritaria di pi\u00f9 (peso \u2248 1/frequenza). Quando: sbilanciamento marcato e interesse a recall migliore senza riscrivere la loss.</p>"},{"location":"quiz/#28-come-controlli-la-stabilita-dei-risultati","title":"28) Come controlli la stabilit\u00e0 dei risultati?","text":"<p>Risposta: Guarda media \u00b1 std in CV (anche ripetuta), prova pi\u00f9 seed, verifica coerenza tra OOF e test. Se la varianza \u00e8 alta, preferisci modelli/parametri pi\u00f9 semplici (regola 1-SE).</p>"},{"location":"quiz/#29-perche-05-non-e-una-legge-per-la-soglia","title":"29) Perch\u00e9 0.5 non \u00e8 una legge per la soglia?","text":"<p>Risposta: 0.5 presuppone costi simmetrici e probabilit\u00e0 perfettamente calibrate. Nel mondo reale FN \u2260 FP: scegli la soglia in base a F1/Youden/costo (e dopo aver calibrato se servono probabilit\u00e0 affidabili).</p>"},{"location":"quiz/#30-workflow-che-non-ti-frega-passo-passo","title":"30) Workflow \u201cche non ti frega\u201d (passo-passo)","text":"<p>1) Split train/test (test in cassaforte). 2) Pipeline con preprocess \u2192 CV \u2192 tuning (regola 1-SE se serve). 3) Soglia su validation/OOF (o per costo). 4) Fit finale su tutto il train (stessa pipeline/parametri/soglia). 5) Valutazione sul test una sola volta. 6) Spiega: importanze/coef, curve ROC/PR, ablation, calibrazione.</p>"},{"location":"quiz/#bonus-come-leggere-un-run-summary-stampato-dagli-script","title":"Bonus) Come leggere un RUN SUMMARY stampato dagli script","text":"<p>Risposta: \u00c8 un estratto a prova d\u2019occhio: seed, soglia, metriche train/test, confusion e CV (media\u00b1std), eventuale costo. Ti dice come hai scelto e quanto puoi fidarti (stabilit\u00e0, onest\u00e0 metodologica).</p>"},{"location":"quiz/#mini-glossario","title":"Mini-glossario","text":"<ul> <li>Parametro: numero interno al modello regolato dall\u2019algoritmo (le \u201cmanopole\u201d).  </li> <li>Iperparametro: numero scelto dall\u2019utente con CV (profondit\u00e0, C, learning rate\u2026).  </li> <li>Loss/Errore: numero che misura \u201cquanto sto sbagliando\u201d durante l\u2019addestramento.  </li> <li>Generalizzazione: andare bene su dati nuovi (test).  </li> <li>Calibrazione: rendere le probabilit\u00e0 coerenti con le frequenze reali.  </li> <li>OOF: predizioni su train ottenute senza vedere quel campione (da altri fold).  </li> <li>Youden: TPR\u2212FPR; criterio di scelta soglia dalla ROC.  </li> <li>Ablation: test di robustezza rimuovendo feature importanti.</li> </ul>"},{"location":"script/","title":"Script inclusi","text":"<p>Questa pagina elenca gli script del laboratorio con cosa fanno, opzioni principali e comandi tipici. Gli script stampano report \u201cparlanti\u201d (metriche, confusion, analisi soglie ecc.). Alcuni includono anche gli appunti finali con <code>--print-cheatsheet</code>.</p>"},{"location":"script/#scriptsirispy","title":"<code>scripts/iris.py</code>","text":"<p>Cosa: Decision Tree sull\u2019IRIS (3 classi). Perch\u00e9: capire over/underfitting con regole leggibili (SE\u2026 ALLORA\u2026).</p> <p>Opzioni principali - <code>--max-depth INT</code> \u2026 limita la profondit\u00e0 dell\u2019albero. - <code>--tune</code> \u2026 sceglie in modo onesto la profondit\u00e0 via k-fold (solo sul train). - <code>--seed INT</code> \u2026 fissa lo split. - <code>--print-cheatsheet</code> \u2026 stampa appunti finali (lesson learned essenziali).</p> <p>Esempi</p> <pre><code>python scripts/iris.py\npython scripts/iris.py --max-depth 3\npython scripts/iris.py --tune --seed 13 --print-cheatsheet\n````\n\n**Cosa vedrai**\n\n* Accuratezza **train/test**\n* **Regole** dell\u2019albero in testo\n* **Confusion matrix** multiclasse\n* **Feature importance**\n* Mini **sweep** di profondit\u00e0 con train vs test\n* (se `--tune`) scelta della profondit\u00e0 via CV\n\n---\n\n## `scripts/imbalance.py`\n\n**Cosa:** Classificazione **sbilanciata** (Breast Cancer) con **Logistic Regression** + **scaling**.\n**Perch\u00e9:** mostrare che l\u2019**accuracy** pu\u00f2 ingannare; serve guardare **Precision/Recall/F1** e **scegliere la soglia**.\n\n**Opzioni principali**\n\n* `--C FLOAT` \u2026 forza del modello logit (C\u2191 = meno regolarizzazione).\n* `--threshold FLOAT` \u2026 soglia decisione (default 0.5).\n* `--auto-threshold` \u2026 sceglie la soglia da **validation** (solo train).\n* `--metric {f1,youden}` \u2026 criterio per l\u2019auto-soglia.\n* `--seed INT`\n* `--print-cheatsheet` \u2026 appunti finali.\n\n**Esempi**\n\n```bash\npython scripts/imbalance.py\npython scripts/imbalance.py --auto-threshold --metric f1 --print-cheatsheet\npython scripts/imbalance.py --C 0.5 --seed 13\n</code></pre> <p>Cosa vedrai</p> <ul> <li>Baseline (classe maggioritaria)</li> <li>Distribuzione degli score (quantili per negativi/positivi)</li> <li>Sweep soglia (tabella soglia \u2192 FP/FN/Precision/Recall/F1)</li> <li>Soglia a costo (es. FN 5\u00d7 FP)</li> <li>Report completo: Accuracy, Precision, Recall, F1, ROC-AUC, Confusion matrix</li> <li>CV ROC-AUC sul train</li> </ul>"},{"location":"script/#scriptsforest_vs_logitpy","title":"<code>scripts/forest_vs_logit.py</code>","text":"<p>Cosa: Confronto Random Forest vs Logistic su Breast Cancer. Include calibrazione delle probabilit\u00e0 RF e scelta soglia separata per i due modelli.</p> <p>Opzioni principali</p> <ul> <li> <p>Soglia:</p> </li> <li> <p><code>--threshold FLOAT</code>, <code>--auto-threshold</code>, <code>--metric {f1,youden}</code></p> </li> <li> <p>Random Forest:</p> </li> <li> <p><code>--rf-n INT</code>, <code>--rf-depth {INT|None}</code>, <code>--rf-mf {sqrt,log2}</code></p> </li> <li><code>--rf-class-weight {balanced|None}</code></li> <li><code>--rf-tune</code> \u2026 grid semplice via CV (sul train)</li> <li><code>--rf-calibrate {isotonic,sigmoid}</code> \u2026 CalibratedClassifierCV su train</li> <li> <p>Logistica:</p> </li> <li> <p><code>--C FLOAT</code></p> </li> <li> <p>Generali:</p> </li> <li> <p><code>--seed INT</code>, <code>--print-cheatsheet</code></p> </li> </ul> <p>Esempi</p> <pre><code>python scripts/forest_vs_logit.py --auto-threshold\npython scripts/forest_vs_logit.py --rf-calibrate isotonic --auto-threshold --print-cheatsheet\npython scripts/forest_vs_logit.py --rf-tune --seed 13\n</code></pre> <p>Cosa vedrai</p> <ul> <li>Baseline maggioritaria</li> <li>Analisi punteggi RF (quantili + sweep soglia)</li> <li>Report logit e RF (Accuracy, F1, ROC-AUC, Confusion)</li> <li>Importanze RF (da un clone non calibrato, se necessario)</li> <li>CV ROC-AUC sul train per entrambi</li> </ul>"},{"location":"script/#scriptsimportance_demopy","title":"<code>scripts/importance_demo.py</code>","text":"<p>Cosa: Importanza delle feature con Random Forest: impurity vs permutation, correlazioni e ablation (drop top-k). Perch\u00e9: non farsi ingannare dalle sole <code>feature_importances_</code>.</p> <p>Opzioni principali</p> <ul> <li><code>--seed INT</code></li> <li><code>--print-cheatsheet</code></li> </ul> <p>Esempi</p> <pre><code>python scripts/importance_demo.py\npython scripts/importance_demo.py --seed 13 --print-cheatsheet\n</code></pre> <p>Cosa vedrai</p> <ul> <li>AUC e Accuracy base della RF</li> <li>Impurity importance (top-k)</li> <li>Permutation importance su TEST (metrica ROC-AUC)</li> <li>Coppie molto correlate (|corr| \u2265 0.9) \u2014 warning interpretativo</li> <li>Ablation: AUC base vs AUC senza top-1 / top-3</li> </ul>"},{"location":"script/#scriptsgridsearch_mixedpy","title":"<code>scripts/gridsearch_mixed.py</code>","text":"<p>Cosa: Dati misti (numeriche + categoriche, con missing) con Pipeline/ColumnTransformer e GridSearchCV (no leakage). Sceglie la soglia onesta da OOF (predizioni out-of-fold). Supporta Logit e RF.</p> <p>Opzioni principali (le pi\u00f9 usate)</p> <ul> <li>Modello: <code>--model {logit,rf}</code></li> <li>Soglia: <code>--auto-threshold</code>, <code>--thr-mode {f1,youden,cost}</code>, <code>--cost-fp FLOAT</code>, <code>--cost-fn FLOAT</code></li> <li>Dati: <code>--missing FLOAT</code> (quota di valori mancanti generati), <code>--seed INT</code></li> <li>(Altre opzioni sono mostrate da <code>--help</code>)</li> </ul> <p>Esempi</p> <pre><code># Logit con soglia OOF (F1)\npython scripts/gridsearch_mixed.py --model logit --auto-threshold\n\n# RF con soglia a costo (FN 10\u00d7 FP)\npython scripts/gridsearch_mixed.py --model rf --auto-threshold --thr-mode cost --cost-fn 10\n</code></pre> <p>Cosa vedrai</p> <ul> <li>Top configurazioni da GridSearch (per F1 CV) + AUC</li> <li>Soglia OOF scelta (criterio F1/Youden/costo)</li> <li>Report TRAIN e TEST alla soglia scelta</li> <li>Per Logit: Top coefficienti (segnano +/\u2212 rischio)   Per RF: Top importanze</li> </ul>"},{"location":"script/#scriptspipeline_leakagepy","title":"<code>scripts/pipeline_leakage.py</code>","text":"<p>Cosa: Dimostrazione leakage: confronto \u201csbagliato\u201d (trasformazioni prima dello split) vs \u201ccorretto\u201d (Pipeline solo sul train).</p> <p>Esempi</p> <pre><code>python scripts/pipeline_leakage.py\n</code></pre> <p>Cosa vedrai</p> <ul> <li>Confronto affiancato: Accuracy e F1</li> <li>CV (train) con media \u00b1 std per la versione corretta</li> </ul>"},{"location":"script/#convenzioni-di-output-rapido-promemoria","title":"Convenzioni di output (rapido promemoria)","text":"<ul> <li>Metriche chiave: Accuracy, Precision, Recall, F1, ROC-AUC</li> <li>Confusion matrix con conteggi TN/FP/FN/TP \u201cin chiaro\u201d</li> <li>Soglia: tabella \u201csweep soglia\u201d e/o scelta auto (validation/OOF) o a costo</li> <li>CV: riportata come media \u00b1 deviazione standard (k-fold sul train)</li> </ul> <p>Tip: se perdi pochi positivi \u00e8 grave, abbassa la soglia; se odi i falsi allarmi, alzala. Con RF valuta calibrazione (<code>isotonic</code>/<code>sigmoid</code>) quando servono probabilit\u00e0 affidabili.</p>"}]}