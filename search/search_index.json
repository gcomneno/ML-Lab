{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ML-Lab \u00b6 Benvenuto! Questo \u00e8 il laboratorio pratico per capire davvero ML: - modelli semplici ma realistici, - soglie (non solo 0.5), - importanze delle feature, - niente leakage (pipeline + CV), - report \u201cparlanti\u201d. \ud83d\udc49 Codice: repo GitHub Usa il menu a sinistra per navigare. Buono studio! \ud83d\udcaa","title":"Home"},{"location":"#ml-lab","text":"Benvenuto! Questo \u00e8 il laboratorio pratico per capire davvero ML: - modelli semplici ma realistici, - soglie (non solo 0.5), - importanze delle feature, - niente leakage (pipeline + CV), - report \u201cparlanti\u201d. \ud83d\udc49 Codice: repo GitHub Usa il menu a sinistra per navigare. Buono studio! \ud83d\udcaa","title":"ML-Lab"},{"location":"lessons-learned/","text":"ML \u2014 Lessons Learned (versione \u201cterra-terra\u201d, aggiornata) \u00b6 Di seguito il riassunto che useremmo davvero in lab: spiegato semplice, con esempi concreti e zero fuffa. 0) Mini-glossario (3 parole che tornano sempre) \u00b6 Parametri : numeri che il modello impara (es. pesi della logistica). Iperparametri : manopole che scegli tu (es. max_depth di un albero, C della logistica). Fit : l\u2019atto di far apprendere il modello dal train . 1) Cosa stiamo facendo \u00b6 ML = imparare una funzione input \u2192 output dai dati. Non scrivi la formula a mano: la stima il modello minimizzando un errore. Metodo onesto: separa sempre i dati in train (si impara) e test (si misura). Il test \u00e8 sacro : non lo usi per scegliere nulla. 2) Fit buono vs fit cattivo \u00b6 Overfitting: vola su train , crolla su test \u2192 ha imparato anche il rumore. Sintomi: modello troppo complesso, regole iper-specifiche. Underfitting: scarso sia su train sia su test \u2192 modello troppo semplice o features povere. Che fare: confronta train vs test , regola la complessit\u00e0 (es. max_depth , C ), aggiungi features/dati migliori. 3) Decision Tree (albero) \u00b6 Pro: regole leggibili ( SE \u2026 ALLORA \u2026 ), spiegazioni facili. Manopole principali: max_depth , min_samples_leaf , max_leaf_nodes . Nota: un singolo albero \u00e8 instabile (alta varianza). La Random Forest media molti alberi e stabilizza. 4) Come scegliere gli iperparametri senza barare \u00b6 k-fold cross-validation sul solo train (es. 5-fold). Media e deviazione ti dicono qualit\u00e0 e incertezza. Regola \u201c1-SE\u201d: se pi\u00f9 settaggi sono quasi pari, scegli il pi\u00f9 semplice . Dataset piccolo? Fai CV ripetuta (es. 5\u00d75-fold) per avere una media pi\u00f9 stabile. 5) Classi sbilanciate: l\u2019accuracy mente \u00b6 Se il 90% \u00e8 classe 0, dire sempre \u201c0\u201d d\u00e0 accuracy 0.90 \u2026 ma non serve a nulla. Guarda Precision (pochi falsi allarmi), Recall (pochi positivi persi), F1 (equilibrio tra i due). Confusion matrix = tavolo della verit\u00e0: TN, FP, FN, TP. Esempio lampo (100 casi): Predici 40 positivi \u2192 30 giusti (TP) e 10 falsi allarmi (FP). Ci sono 35 positivi reali: ne hai persi 5 (FN). Precision = 30/(30+10)=0.75 Recall = 30/(30+5)=0.86 F1 \u2248 0.80 6) F1 e ROC-AUC in due righe \u00b6 F1: numero unico che sale solo se precision e recall sono entrambi alti. F1 = 2 \u00b7 (Prec\u00b7Rec) / (Prec + Rec) ROC-AUC: qualit\u00e0 del ranking delle probabilit\u00e0 su tutte le soglie. 0.5 = a caso, 1.0 = perfetto. Non dipende dalla soglia 0.5. Nota: con sbilanciamenti molto forti, guarda anche la PR-AUC (area Precision-Recall). 7) La soglia: 0.5 non \u00e8 legge \u00b6 Cambiare soglia scambia FP \u2194 FN . Tre modi pratici per sceglierla: Auto-threshold su validation (dal train ): massimizza F1 (o Youden TPR\u2212FPR ). A costo: scegli soglia che minimizza c_fp\u00b7FP + c_fn\u00b7FN (decidi tu quanto pesa un FN vs FP). Sweep tabellare: stampa soglia \u2192 (FP, FN, Precision, Recall, F1) e guardala. Regola pratica: Se perdere un positivo fa male \u2192 soglia pi\u00f9 bassa (recall su, pi\u00f9 FP). Se odi i falsi allarmi \u2192 soglia pi\u00f9 alta (precision su, pi\u00f9 FN). 8) Scaling: quando s\u00ec e quando no \u00b6 Serve a modelli basati su distanze/geometria: Logistica , SVM , k-NN , PCA . Non serve in genere per Alberi/Random Forest (si basano su soglie, non su distanze). 9) Random Forest: perch\u00e9 funziona e dove punge \u00b6 Pro: cattura non-linearit\u00e0 e interazioni , robusta al rumore, buona \u201cout-of-the-box\u201d. Contro tipico: le probabilit\u00e0 possono essere poco calibrate \u2192 0.5 spesso non \u00e8 la soglia giusta. Rimedi: scegli bene la soglia (auto/costo/sweep), valuta class_weight='balanced' se la positiva \u00e8 rara, calibra le probabilit\u00e0 ( sigmoid/Platt o isotonic ) se ti servono score affidabili. 10) \u201cImportanza\u201d delle feature: evitare abbagli \u00b6 Impurity importance ( feature_importances_ ): rapida, ma favorisce variabili con molte soglie e soffre con feature molto correlate (le \u201csorelle\u201d si dividono il merito). Permutation importance (su TEST e con una metrica, es. ROC-AUC): misuri quanto peggiora il modello quando rompi una feature \u2192 spesso pi\u00f9 onesta. Ablation test: rimuovi la top-1/top-k e rimisura. Se l\u2019AUC non scende, c\u2019\u00e8 ridondanza (pi\u00f9 feature dicono la stessa cosa). 11) Leakage: barare senza volerlo \u00b6 Cos\u2019\u00e8: usare info del test (o del fold di validazione) per calcolare imputazioni, scaling, encoding, selezione feature\u2026 Risultato: metriche gonfiate. Regola d\u2019oro: tutte le trasformazioni si fittano solo sul train . Strumenti che ti proteggono: Pipeline (e ColumnTransformer per numeriche+categoriche) + GridSearchCV sulla pipeline . Spia rossa: CV troppo bella dopo aver preprocessato prima della CV. 12) Workflow che non ti tradisce \u00b6 Split train/test (il test in cassaforte). Sul train : costruisci Pipeline (preprocess + modello). CV (anche ripetuta) per tuning iperparametri (regola 1-SE se pari). Soglia : scegli su validation/OOF o per costo . Fit finale sul train completo con pipeline + iperparametri + soglia scelti. Test una sola volta : F1, ROC-AUC, confusion; se serve, calibrazione (Brier, reliability bins). Spiega : importanze (impurity + permutation) e, se serve, ablation. 13) Regole veloci da campo \u00b6 Mai usare il test per scegliere iperparametri o soglia. Dataset piccolo \u2192 CV ripetuta + regola 1-SE . Sbilanciamento: valuta F1 e/o Recall (se i FN costano), usa ROC-AUC per confrontare modelli e PR-AUC se lo sbilanciamento \u00e8 estremo. RF: non fissarti su 0.5; scegli soglia e valuta calibrazione . Logistica: con scaling, spesso imbattibile su confini quasi lineari; probabilit\u00e0 molto affidabili . 14) Debug veloce (checklist) \u00b6 Baseline : quanto fa un dummy che predice la classe pi\u00f9 frequente? Leakage : sto facendo imputazione/scaling fuori pipeline? Metriche giuste : sto guardando F1/Recall oltre all\u2019accuracy? Soglia : ho provato auto-threshold o a costo? Varianza : risultati stabili cambiando seed ? (se no, CV ripetuta) Ridondanza : feature molto correlate? Usa permutation + ablation.","title":"Lessons Learned"},{"location":"lessons-learned/#ml-lessons-learned-versione-terra-terra-aggiornata","text":"Di seguito il riassunto che useremmo davvero in lab: spiegato semplice, con esempi concreti e zero fuffa.","title":"ML \u2014 Lessons Learned (versione \u201cterra-terra\u201d, aggiornata)"},{"location":"lessons-learned/#0-mini-glossario-3-parole-che-tornano-sempre","text":"Parametri : numeri che il modello impara (es. pesi della logistica). Iperparametri : manopole che scegli tu (es. max_depth di un albero, C della logistica). Fit : l\u2019atto di far apprendere il modello dal train .","title":"0) Mini-glossario (3 parole che tornano sempre)"},{"location":"lessons-learned/#1-cosa-stiamo-facendo","text":"ML = imparare una funzione input \u2192 output dai dati. Non scrivi la formula a mano: la stima il modello minimizzando un errore. Metodo onesto: separa sempre i dati in train (si impara) e test (si misura). Il test \u00e8 sacro : non lo usi per scegliere nulla.","title":"1) Cosa stiamo facendo"},{"location":"lessons-learned/#2-fit-buono-vs-fit-cattivo","text":"Overfitting: vola su train , crolla su test \u2192 ha imparato anche il rumore. Sintomi: modello troppo complesso, regole iper-specifiche. Underfitting: scarso sia su train sia su test \u2192 modello troppo semplice o features povere. Che fare: confronta train vs test , regola la complessit\u00e0 (es. max_depth , C ), aggiungi features/dati migliori.","title":"2) Fit buono vs fit cattivo"},{"location":"lessons-learned/#3-decision-tree-albero","text":"Pro: regole leggibili ( SE \u2026 ALLORA \u2026 ), spiegazioni facili. Manopole principali: max_depth , min_samples_leaf , max_leaf_nodes . Nota: un singolo albero \u00e8 instabile (alta varianza). La Random Forest media molti alberi e stabilizza.","title":"3) Decision Tree (albero)"},{"location":"lessons-learned/#4-come-scegliere-gli-iperparametri-senza-barare","text":"k-fold cross-validation sul solo train (es. 5-fold). Media e deviazione ti dicono qualit\u00e0 e incertezza. Regola \u201c1-SE\u201d: se pi\u00f9 settaggi sono quasi pari, scegli il pi\u00f9 semplice . Dataset piccolo? Fai CV ripetuta (es. 5\u00d75-fold) per avere una media pi\u00f9 stabile.","title":"4) Come scegliere gli iperparametri senza barare"},{"location":"lessons-learned/#5-classi-sbilanciate-laccuracy-mente","text":"Se il 90% \u00e8 classe 0, dire sempre \u201c0\u201d d\u00e0 accuracy 0.90 \u2026 ma non serve a nulla. Guarda Precision (pochi falsi allarmi), Recall (pochi positivi persi), F1 (equilibrio tra i due). Confusion matrix = tavolo della verit\u00e0: TN, FP, FN, TP. Esempio lampo (100 casi): Predici 40 positivi \u2192 30 giusti (TP) e 10 falsi allarmi (FP). Ci sono 35 positivi reali: ne hai persi 5 (FN). Precision = 30/(30+10)=0.75 Recall = 30/(30+5)=0.86 F1 \u2248 0.80","title":"5) Classi sbilanciate: l\u2019accuracy mente"},{"location":"lessons-learned/#6-f1-e-roc-auc-in-due-righe","text":"F1: numero unico che sale solo se precision e recall sono entrambi alti. F1 = 2 \u00b7 (Prec\u00b7Rec) / (Prec + Rec) ROC-AUC: qualit\u00e0 del ranking delle probabilit\u00e0 su tutte le soglie. 0.5 = a caso, 1.0 = perfetto. Non dipende dalla soglia 0.5. Nota: con sbilanciamenti molto forti, guarda anche la PR-AUC (area Precision-Recall).","title":"6) F1 e ROC-AUC in due righe"},{"location":"lessons-learned/#7-la-soglia-05-non-e-legge","text":"Cambiare soglia scambia FP \u2194 FN . Tre modi pratici per sceglierla: Auto-threshold su validation (dal train ): massimizza F1 (o Youden TPR\u2212FPR ). A costo: scegli soglia che minimizza c_fp\u00b7FP + c_fn\u00b7FN (decidi tu quanto pesa un FN vs FP). Sweep tabellare: stampa soglia \u2192 (FP, FN, Precision, Recall, F1) e guardala. Regola pratica: Se perdere un positivo fa male \u2192 soglia pi\u00f9 bassa (recall su, pi\u00f9 FP). Se odi i falsi allarmi \u2192 soglia pi\u00f9 alta (precision su, pi\u00f9 FN).","title":"7) La soglia: 0.5 non \u00e8 legge"},{"location":"lessons-learned/#8-scaling-quando-si-e-quando-no","text":"Serve a modelli basati su distanze/geometria: Logistica , SVM , k-NN , PCA . Non serve in genere per Alberi/Random Forest (si basano su soglie, non su distanze).","title":"8) Scaling: quando s\u00ec e quando no"},{"location":"lessons-learned/#9-random-forest-perche-funziona-e-dove-punge","text":"Pro: cattura non-linearit\u00e0 e interazioni , robusta al rumore, buona \u201cout-of-the-box\u201d. Contro tipico: le probabilit\u00e0 possono essere poco calibrate \u2192 0.5 spesso non \u00e8 la soglia giusta. Rimedi: scegli bene la soglia (auto/costo/sweep), valuta class_weight='balanced' se la positiva \u00e8 rara, calibra le probabilit\u00e0 ( sigmoid/Platt o isotonic ) se ti servono score affidabili.","title":"9) Random Forest: perch\u00e9 funziona e dove punge"},{"location":"lessons-learned/#10-importanza-delle-feature-evitare-abbagli","text":"Impurity importance ( feature_importances_ ): rapida, ma favorisce variabili con molte soglie e soffre con feature molto correlate (le \u201csorelle\u201d si dividono il merito). Permutation importance (su TEST e con una metrica, es. ROC-AUC): misuri quanto peggiora il modello quando rompi una feature \u2192 spesso pi\u00f9 onesta. Ablation test: rimuovi la top-1/top-k e rimisura. Se l\u2019AUC non scende, c\u2019\u00e8 ridondanza (pi\u00f9 feature dicono la stessa cosa).","title":"10) \u201cImportanza\u201d delle feature: evitare abbagli"},{"location":"lessons-learned/#11-leakage-barare-senza-volerlo","text":"Cos\u2019\u00e8: usare info del test (o del fold di validazione) per calcolare imputazioni, scaling, encoding, selezione feature\u2026 Risultato: metriche gonfiate. Regola d\u2019oro: tutte le trasformazioni si fittano solo sul train . Strumenti che ti proteggono: Pipeline (e ColumnTransformer per numeriche+categoriche) + GridSearchCV sulla pipeline . Spia rossa: CV troppo bella dopo aver preprocessato prima della CV.","title":"11) Leakage: barare senza volerlo"},{"location":"lessons-learned/#12-workflow-che-non-ti-tradisce","text":"Split train/test (il test in cassaforte). Sul train : costruisci Pipeline (preprocess + modello). CV (anche ripetuta) per tuning iperparametri (regola 1-SE se pari). Soglia : scegli su validation/OOF o per costo . Fit finale sul train completo con pipeline + iperparametri + soglia scelti. Test una sola volta : F1, ROC-AUC, confusion; se serve, calibrazione (Brier, reliability bins). Spiega : importanze (impurity + permutation) e, se serve, ablation.","title":"12) Workflow che non ti tradisce"},{"location":"lessons-learned/#13-regole-veloci-da-campo","text":"Mai usare il test per scegliere iperparametri o soglia. Dataset piccolo \u2192 CV ripetuta + regola 1-SE . Sbilanciamento: valuta F1 e/o Recall (se i FN costano), usa ROC-AUC per confrontare modelli e PR-AUC se lo sbilanciamento \u00e8 estremo. RF: non fissarti su 0.5; scegli soglia e valuta calibrazione . Logistica: con scaling, spesso imbattibile su confini quasi lineari; probabilit\u00e0 molto affidabili .","title":"13) Regole veloci da campo"},{"location":"lessons-learned/#14-debug-veloce-checklist","text":"Baseline : quanto fa un dummy che predice la classe pi\u00f9 frequente? Leakage : sto facendo imputazione/scaling fuori pipeline? Metriche giuste : sto guardando F1/Recall oltre all\u2019accuracy? Soglia : ho provato auto-threshold o a costo? Varianza : risultati stabili cambiando seed ? (se no, CV ripetuta) Ridondanza : feature molto correlate? Usa permutation + ablation.","title":"14) Debug veloce (checklist)"},{"location":"quiz/","text":"ML-Lab \u2014 Quiz Q&A (versione chiara) \u00b6 Studio autonomo \u2014 domande & risposte semplici ma dettagliate. Consiglio: prova prima a rispondere da solo, poi confronta con la soluzione. Ogni risposta include il \u201cperch\u00e9\u201d e spesso un mini-esempio pratico. 1) Che cos\u2019\u00e8, in una frase, il Machine Learning? \u00b6 Risposta (umana): Costruire un programma/funzione che impara una determinata regola dai dati : gli mostri esempi di input e il risultato giusto , e lui regola da solo le sue manopole per sbagliare il meno possibile sugli esempi, puntando ad andare bene anche su dati nuovi . - Parametri = manopole del modello (numeri). - Algoritmo di apprendimento = come giriamo le manopole per ridurre l\u2019 errore . - Generalizzazione = andare bene su test nuovi, su dati mai visti. 2) Perch\u00e9 il \u201ctest \u00e8 sacro\u201d? \u00b6 Risposta: Serve a misurare la generalizzazione vera . Se lo usi per scegliere iperparametri (o le soglie), inquini la misura (stai \u201cimparando\u201d anche dal test ed \u00e8 scorretto). Regola: tutte le decisioni (tuning, soglia, trasformazioni) vanno prese solo col train (CV/validation). Il test si usa una volta alla fine . 3) Overfitting vs Underfitting: differenza pratica e rimedi \u00b6 Overfit: ottimo su train , peggio su test \u2192 troppo complesso / ha \u201cpappagallato\u201d. Underfit: scarso su train e test \u2192 troppo semplice / info insufficienti. Rimedi: riduci o aumenti la complessit\u00e0 (es. max_depth , C ), aggiungi dati/feature, regolarizzazione, semplifica il modello. 4) Decision Tree: perch\u00e9 \u00e8 spiegabile e come ne controlli la complessit\u00e0? \u00b6 Risposta: L\u2019albero produce regole SE \u2026 ALLORA \u2026 leggibili (threshold sulle feature). Controllo: max_depth , min_samples_leaf , max_leaf_nodes . Nota: un albero singolo ha alta varianza \u2192 piccole variazioni nei dati cambiano molto la struttura. 5) Perch\u00e9 una Random Forest riduce la varianza dell\u2019albero singolo? \u00b6 Risposta: Fa la media di molti alberi addestrati su campioni diversi e sottoinsiemi di feature (bagging + casualit\u00e0). La media stabilizza e riduce errori \u201ccapricciosi\u201d. 6) Cos\u2019\u00e8 un iperparametro? Esempi \u00b6 Risposta: \u00c8 un controllo del modello che non viene imparato dai dati ma scelto dall\u2019utente con CV. Esempi: max_depth (Tree/RF), n_estimators (RF), C (Logit/SVM), max_features (RF), learning_rate (boosting). 7) k-fold Cross-Validation: cos\u2019\u00e8 e perch\u00e9 sul solo train? \u00b6 Risposta: Dividi il train in k parti; ripeti k volte: alleni su k-1 parti, valuti sulla parte rimasta. Media e deviazione standard ti danno una stima stabile . Si fa solo sul train per non \u201cguardare\u201d il test . 8) Regola 1-SE: quando e perch\u00e9? \u00b6 Risposta: Se pi\u00f9 modelli sono statisticamente equivalenti (entro 1 deviazione standard dalla migliore metrica), scegli la versione pi\u00f9 semplice . \u00c8 pi\u00f9 robusta su dati nuovi. 9) Perch\u00e9 l\u2019accuracy pu\u00f2 mentire con classi sbilanciate? \u00b6 Risposta: Se i positivi sono rari, dire \u201csempre negativo\u201d pu\u00f2 dare alta accuracy ma recall=0 . Soluzione: guarda Precision , Recall , F1 e la Confusion matrix . 10) Precision, Recall, F1: definizioni pratiche \u00b6 Precision = tra gli allarmi che hai dato, quanti erano giusti ? = TP/(TP+FP) Recall = tra i positivi veri , quanti non hai perso ? = TP/(TP+FN) F1 = media armonica di Precision e Recall \u2192 sale solo se entrambi sono alti. Quando usarle: problemi sbilanciati o quando i costi di FP/FN sono diversi. 11) ROC-AUC: cosa misura e perch\u00e9 non dipende da 0.5 \u00b6 Risposta: Valuta la qualit\u00e0 del ranking delle probabilit\u00e0 su tutte le soglie . Interpretazione: probabilit\u00e0 che un positivo abbia score > di un negativo. 0.5 \u2248 a caso, 1.0 perfetto. Non usa la soglia fissa 0.5. 12) Come si sceglie la soglia? Tre strade \u00b6 1) Auto-threshold su validation/OOF per massimizzare F1 o Youden = TPR\u2212FPR . 2) A costo : minimizza c_fp\u00b7FP + c_fn\u00b7FN (decidi tu quanto pesa perdere un positivo). 3) Sweep : stampi la tabella soglia \u2192 (FP, FN, Precision, Recall, F1) e scegli consapevolmente. Regola pratica: se FN \u00e8 gravissimo , abbassa la soglia. 13) Cosa sono Youden e OOF? \u00b6 Youden J: TPR \u2212 FPR \u2192 cerca la soglia pi\u00f9 distante dalla diagonale della ROC. OOF (Out-Of-Fold): predizioni di CV sul train , fatte da modelli che non hanno visto quel campione. Servono per scelte oneste (soglia, calibrazione) senza toccare il test. 14) Quando serve lo scaling? \u00b6 Risposta: Serve per modelli che usano distanze o geometria (Logit, SVM, k-NN, PCA). In genere non serve per Tree/Random Forest (lavorano per soglie, non distanze). 15) Logistic Regression: pro/contro e iper chiave \u00b6 Pro: semplice, veloce, probabilit\u00e0 ben calibrate , ottima su confini quasi lineari . Contro: fatica con forte non-linearit\u00e0 se non aggiungi feature/interazioni. Iper: C (meno regolarizzazione se alto) \u2192 occhio all\u2019overfit; scalare le feature \u00e8 essenziale. 16) Random Forest: pro/contro e pomelli \u00b6 Pro: coglie non-linearit\u00e0 e interazioni , robusta, pochi settaggi critici. Contro: probabilit\u00e0 spesso non calibrate ; spiegabilit\u00e0 globale limitata. Pomelli: n_estimators , max_depth , max_features , class_weight per sbilanciamento. 17) Perch\u00e9 calibrare le probabilit\u00e0 di una RF? Come? \u00b6 Risposta: Perch\u00e9 gli score possono essere troppo ottimisti/piatti . Con CalibratedClassifierCV : - sigmoid (Platt): semplice, pochi dati. - isotonic: pi\u00f9 flessibile, richiede pi\u00f9 dati. Trucco pratico: per leggere feature_importances_ quando usi la calibrazione, rifitta un clone della RF non calibrata sul train. 18) Importanza delle feature: \u201cimpurity\u201d vs \u201cpermutation\u201d \u00b6 Impurity (feature_importances_) : veloce, ma favorisce feature con tante soglie; con feature correlate si divide il merito . Permutation (su TEST) : misura il calo reale di metrica quando \u201crompi\u201d la feature \u2192 spesso pi\u00f9 onesta . Uso consigliato: guarda entrambe e conferma con ablation . 19) Effetto delle feature molto correlate \u00b6 Risposta: Se due feature dicono la stessa cosa , l\u2019importanza si spalma su entrambe: nessuna sembra \u201cdominare\u201d da sola. Cosa fare: controlla correlazioni (|\u03c1|\u22650.9), prova ablation , valuta grouping /RIDUZIONE (PCA o scelta di una sola). 20) Cos\u2019\u00e8 l\u2019ablation test? \u00b6 Risposta: Togli le top-1/top-k feature e misuri quanto scende la metrica (es. AUC). Lettura: se non scende, il segnale era ridondante ; se crolla, quelle feature sono cruciali . 21) Cos\u2019\u00e8 il data leakage? Esempio e prevenzione \u00b6 Leakage: usi info del futuro/test nell\u2019addestramento. Esempio: scalare o imputare sull\u2019 intero dataset prima dello split/CV. Prevenzione: tutte le trasformazioni in Pipeline/ColumnTransformer , cos\u00ec ogni fold fitta solo sul proprio train . 22) Perch\u00e9 Pipeline + GridSearchCV previene il leakage? \u00b6 Risposta: Perch\u00e9 imputazione/scaling/encoding e modello sono insieme nella pipeline e vengono fittati dentro la CV solo sui dati di train del fold. Il fold di validazione (e il test finale) restano puliti . 23) Predizioni OOF: perch\u00e9 usarle per la soglia? \u00b6 Risposta: Sono predizioni su campioni mai visti dal rispettivo modello di fold \u2192 sono oneste come un mini-test interno. Permettono di scegliere soglie/calibrazioni senza toccare il test. 24) \u201cRefit su F1\u201d: cosa vuol dire e come valuti il test? \u00b6 Risposta: In GridSearchCV(refit=\"f1\") , dopo la CV si ri-addestra il miglior modello (per F1) su tutto il train . Poi si valuta una sola volta sul test (niente ulteriori scelte sul test). 25) Confusion matrix: lettura e collegamento ai costi \u00b6 TN: negativi corretti; FP: falsi allarmi; FN: positivi persi; TP: positivi presi. Costo totale: c_fp\u00b7FP + c_fn\u00b7FN . Soglia a costo: scegli la soglia che minimizza questo valore, in base al tuo scenario. 26) Logit o Random Forest? Quando scegliere cosa \u00b6 Logit: confini lineari , bisogno di probabilit\u00e0 affidabili , spiegabilit\u00e0 dei coefficienti . RF: dati non-lineari , interazioni, \u201cout-of-the-box\u201d forte; poi regola soglia e, se servono probabilit\u00e0 affidabili, calibra . 27) class_weight='balanced' : cosa fa e quando usarlo? \u00b6 Risposta: Pesa gli errori della classe minoritaria di pi\u00f9 (peso \u2248 1/frequenza). Quando: sbilanciamento marcato e interesse a recall migliore senza riscrivere la loss. 28) Come controlli la stabilit\u00e0 dei risultati? \u00b6 Risposta: Guarda media \u00b1 std in CV (anche ripetuta), prova pi\u00f9 seed , verifica coerenza tra OOF e test . Se la varianza \u00e8 alta, preferisci modelli/parametri pi\u00f9 semplici (regola 1-SE). 29) Perch\u00e9 0.5 non \u00e8 una legge per la soglia? \u00b6 Risposta: 0.5 presuppone costi simmetrici e probabilit\u00e0 perfettamente calibrate. Nel mondo reale FN \u2260 FP : scegli la soglia in base a F1/Youden/costo (e dopo aver calibrato se servono probabilit\u00e0 affidabili). 30) Workflow \u201cche non ti frega\u201d (passo-passo) \u00b6 1) Split train/test (test in cassaforte ). 2) Pipeline con preprocess \u2192 CV \u2192 tuning (regola 1-SE se serve). 3) Soglia su validation/OOF (o per costo ). 4) Fit finale su tutto il train (stessa pipeline/parametri/soglia). 5) Valutazione sul test una sola volta. 6) Spiega : importanze/coef, curve ROC/PR, ablation, calibrazione. Bonus) Come leggere un RUN SUMMARY stampato dagli script \u00b6 Risposta: \u00c8 un estratto a prova d\u2019occhio : seed, soglia, metriche train/test , confusion e CV (media\u00b1std) , eventuale costo . Ti dice come hai scelto e quanto puoi fidarti (stabilit\u00e0, onest\u00e0 metodologica). Mini-glossario \u00b6 Parametro: numero interno al modello regolato dall\u2019algoritmo (le \u201cmanopole\u201d). Iperparametro: numero scelto dall\u2019utente con CV (profondit\u00e0, C, learning rate\u2026). Loss/Errore: numero che misura \u201cquanto sto sbagliando\u201d durante l\u2019addestramento. Generalizzazione: andare bene su dati nuovi (test). Calibrazione: rendere le probabilit\u00e0 coerenti con le frequenze reali. OOF: predizioni su train ottenute senza vedere quel campione (da altri fold). Youden: TPR\u2212FPR; criterio di scelta soglia dalla ROC. Ablation: test di robustezza rimuovendo feature importanti.","title":"Quiz"},{"location":"quiz/#ml-lab-quiz-qa-versione-chiara","text":"Studio autonomo \u2014 domande & risposte semplici ma dettagliate. Consiglio: prova prima a rispondere da solo, poi confronta con la soluzione. Ogni risposta include il \u201cperch\u00e9\u201d e spesso un mini-esempio pratico.","title":"ML-Lab \u2014 Quiz Q&amp;A (versione chiara)"},{"location":"quiz/#1-che-cose-in-una-frase-il-machine-learning","text":"Risposta (umana): Costruire un programma/funzione che impara una determinata regola dai dati : gli mostri esempi di input e il risultato giusto , e lui regola da solo le sue manopole per sbagliare il meno possibile sugli esempi, puntando ad andare bene anche su dati nuovi . - Parametri = manopole del modello (numeri). - Algoritmo di apprendimento = come giriamo le manopole per ridurre l\u2019 errore . - Generalizzazione = andare bene su test nuovi, su dati mai visti.","title":"1) Che cos\u2019\u00e8, in una frase, il Machine Learning?"},{"location":"quiz/#2-perche-il-test-e-sacro","text":"Risposta: Serve a misurare la generalizzazione vera . Se lo usi per scegliere iperparametri (o le soglie), inquini la misura (stai \u201cimparando\u201d anche dal test ed \u00e8 scorretto). Regola: tutte le decisioni (tuning, soglia, trasformazioni) vanno prese solo col train (CV/validation). Il test si usa una volta alla fine .","title":"2) Perch\u00e9 il \u201ctest \u00e8 sacro\u201d?"},{"location":"quiz/#3-overfitting-vs-underfitting-differenza-pratica-e-rimedi","text":"Overfit: ottimo su train , peggio su test \u2192 troppo complesso / ha \u201cpappagallato\u201d. Underfit: scarso su train e test \u2192 troppo semplice / info insufficienti. Rimedi: riduci o aumenti la complessit\u00e0 (es. max_depth , C ), aggiungi dati/feature, regolarizzazione, semplifica il modello.","title":"3) Overfitting vs Underfitting: differenza pratica e rimedi"},{"location":"quiz/#4-decision-tree-perche-e-spiegabile-e-come-ne-controlli-la-complessita","text":"Risposta: L\u2019albero produce regole SE \u2026 ALLORA \u2026 leggibili (threshold sulle feature). Controllo: max_depth , min_samples_leaf , max_leaf_nodes . Nota: un albero singolo ha alta varianza \u2192 piccole variazioni nei dati cambiano molto la struttura.","title":"4) Decision Tree: perch\u00e9 \u00e8 spiegabile e come ne controlli la complessit\u00e0?"},{"location":"quiz/#5-perche-una-random-forest-riduce-la-varianza-dellalbero-singolo","text":"Risposta: Fa la media di molti alberi addestrati su campioni diversi e sottoinsiemi di feature (bagging + casualit\u00e0). La media stabilizza e riduce errori \u201ccapricciosi\u201d.","title":"5) Perch\u00e9 una Random Forest riduce la varianza dell\u2019albero singolo?"},{"location":"quiz/#6-cose-un-iperparametro-esempi","text":"Risposta: \u00c8 un controllo del modello che non viene imparato dai dati ma scelto dall\u2019utente con CV. Esempi: max_depth (Tree/RF), n_estimators (RF), C (Logit/SVM), max_features (RF), learning_rate (boosting).","title":"6) Cos\u2019\u00e8 un iperparametro? Esempi"},{"location":"quiz/#7-k-fold-cross-validation-cose-e-perche-sul-solo-train","text":"Risposta: Dividi il train in k parti; ripeti k volte: alleni su k-1 parti, valuti sulla parte rimasta. Media e deviazione standard ti danno una stima stabile . Si fa solo sul train per non \u201cguardare\u201d il test .","title":"7) k-fold Cross-Validation: cos\u2019\u00e8 e perch\u00e9 sul solo train?"},{"location":"quiz/#8-regola-1-se-quando-e-perche","text":"Risposta: Se pi\u00f9 modelli sono statisticamente equivalenti (entro 1 deviazione standard dalla migliore metrica), scegli la versione pi\u00f9 semplice . \u00c8 pi\u00f9 robusta su dati nuovi.","title":"8) Regola 1-SE: quando e perch\u00e9?"},{"location":"quiz/#9-perche-laccuracy-puo-mentire-con-classi-sbilanciate","text":"Risposta: Se i positivi sono rari, dire \u201csempre negativo\u201d pu\u00f2 dare alta accuracy ma recall=0 . Soluzione: guarda Precision , Recall , F1 e la Confusion matrix .","title":"9) Perch\u00e9 l\u2019accuracy pu\u00f2 mentire con classi sbilanciate?"},{"location":"quiz/#10-precision-recall-f1-definizioni-pratiche","text":"Precision = tra gli allarmi che hai dato, quanti erano giusti ? = TP/(TP+FP) Recall = tra i positivi veri , quanti non hai perso ? = TP/(TP+FN) F1 = media armonica di Precision e Recall \u2192 sale solo se entrambi sono alti. Quando usarle: problemi sbilanciati o quando i costi di FP/FN sono diversi.","title":"10) Precision, Recall, F1: definizioni pratiche"},{"location":"quiz/#11-roc-auc-cosa-misura-e-perche-non-dipende-da-05","text":"Risposta: Valuta la qualit\u00e0 del ranking delle probabilit\u00e0 su tutte le soglie . Interpretazione: probabilit\u00e0 che un positivo abbia score > di un negativo. 0.5 \u2248 a caso, 1.0 perfetto. Non usa la soglia fissa 0.5.","title":"11) ROC-AUC: cosa misura e perch\u00e9 non dipende da 0.5"},{"location":"quiz/#12-come-si-sceglie-la-soglia-tre-strade","text":"1) Auto-threshold su validation/OOF per massimizzare F1 o Youden = TPR\u2212FPR . 2) A costo : minimizza c_fp\u00b7FP + c_fn\u00b7FN (decidi tu quanto pesa perdere un positivo). 3) Sweep : stampi la tabella soglia \u2192 (FP, FN, Precision, Recall, F1) e scegli consapevolmente. Regola pratica: se FN \u00e8 gravissimo , abbassa la soglia.","title":"12) Come si sceglie la soglia? Tre strade"},{"location":"quiz/#13-cosa-sono-youden-e-oof","text":"Youden J: TPR \u2212 FPR \u2192 cerca la soglia pi\u00f9 distante dalla diagonale della ROC. OOF (Out-Of-Fold): predizioni di CV sul train , fatte da modelli che non hanno visto quel campione. Servono per scelte oneste (soglia, calibrazione) senza toccare il test.","title":"13) Cosa sono Youden e OOF?"},{"location":"quiz/#14-quando-serve-lo-scaling","text":"Risposta: Serve per modelli che usano distanze o geometria (Logit, SVM, k-NN, PCA). In genere non serve per Tree/Random Forest (lavorano per soglie, non distanze).","title":"14) Quando serve lo scaling?"},{"location":"quiz/#15-logistic-regression-procontro-e-iper-chiave","text":"Pro: semplice, veloce, probabilit\u00e0 ben calibrate , ottima su confini quasi lineari . Contro: fatica con forte non-linearit\u00e0 se non aggiungi feature/interazioni. Iper: C (meno regolarizzazione se alto) \u2192 occhio all\u2019overfit; scalare le feature \u00e8 essenziale.","title":"15) Logistic Regression: pro/contro e iper chiave"},{"location":"quiz/#16-random-forest-procontro-e-pomelli","text":"Pro: coglie non-linearit\u00e0 e interazioni , robusta, pochi settaggi critici. Contro: probabilit\u00e0 spesso non calibrate ; spiegabilit\u00e0 globale limitata. Pomelli: n_estimators , max_depth , max_features , class_weight per sbilanciamento.","title":"16) Random Forest: pro/contro e pomelli"},{"location":"quiz/#17-perche-calibrare-le-probabilita-di-una-rf-come","text":"Risposta: Perch\u00e9 gli score possono essere troppo ottimisti/piatti . Con CalibratedClassifierCV : - sigmoid (Platt): semplice, pochi dati. - isotonic: pi\u00f9 flessibile, richiede pi\u00f9 dati. Trucco pratico: per leggere feature_importances_ quando usi la calibrazione, rifitta un clone della RF non calibrata sul train.","title":"17) Perch\u00e9 calibrare le probabilit\u00e0 di una RF? Come?"},{"location":"quiz/#18-importanza-delle-feature-impurity-vs-permutation","text":"Impurity (feature_importances_) : veloce, ma favorisce feature con tante soglie; con feature correlate si divide il merito . Permutation (su TEST) : misura il calo reale di metrica quando \u201crompi\u201d la feature \u2192 spesso pi\u00f9 onesta . Uso consigliato: guarda entrambe e conferma con ablation .","title":"18) Importanza delle feature: \u201cimpurity\u201d vs \u201cpermutation\u201d"},{"location":"quiz/#19-effetto-delle-feature-molto-correlate","text":"Risposta: Se due feature dicono la stessa cosa , l\u2019importanza si spalma su entrambe: nessuna sembra \u201cdominare\u201d da sola. Cosa fare: controlla correlazioni (|\u03c1|\u22650.9), prova ablation , valuta grouping /RIDUZIONE (PCA o scelta di una sola).","title":"19) Effetto delle feature molto correlate"},{"location":"quiz/#20-cose-lablation-test","text":"Risposta: Togli le top-1/top-k feature e misuri quanto scende la metrica (es. AUC). Lettura: se non scende, il segnale era ridondante ; se crolla, quelle feature sono cruciali .","title":"20) Cos\u2019\u00e8 l\u2019ablation test?"},{"location":"quiz/#21-cose-il-data-leakage-esempio-e-prevenzione","text":"Leakage: usi info del futuro/test nell\u2019addestramento. Esempio: scalare o imputare sull\u2019 intero dataset prima dello split/CV. Prevenzione: tutte le trasformazioni in Pipeline/ColumnTransformer , cos\u00ec ogni fold fitta solo sul proprio train .","title":"21) Cos\u2019\u00e8 il data leakage? Esempio e prevenzione"},{"location":"quiz/#22-perche-pipeline-gridsearchcv-previene-il-leakage","text":"Risposta: Perch\u00e9 imputazione/scaling/encoding e modello sono insieme nella pipeline e vengono fittati dentro la CV solo sui dati di train del fold. Il fold di validazione (e il test finale) restano puliti .","title":"22) Perch\u00e9 Pipeline + GridSearchCV previene il leakage?"},{"location":"quiz/#23-predizioni-oof-perche-usarle-per-la-soglia","text":"Risposta: Sono predizioni su campioni mai visti dal rispettivo modello di fold \u2192 sono oneste come un mini-test interno. Permettono di scegliere soglie/calibrazioni senza toccare il test.","title":"23) Predizioni OOF: perch\u00e9 usarle per la soglia?"},{"location":"quiz/#24-refit-su-f1-cosa-vuol-dire-e-come-valuti-il-test","text":"Risposta: In GridSearchCV(refit=\"f1\") , dopo la CV si ri-addestra il miglior modello (per F1) su tutto il train . Poi si valuta una sola volta sul test (niente ulteriori scelte sul test).","title":"24) \u201cRefit su F1\u201d: cosa vuol dire e come valuti il test?"},{"location":"quiz/#25-confusion-matrix-lettura-e-collegamento-ai-costi","text":"TN: negativi corretti; FP: falsi allarmi; FN: positivi persi; TP: positivi presi. Costo totale: c_fp\u00b7FP + c_fn\u00b7FN . Soglia a costo: scegli la soglia che minimizza questo valore, in base al tuo scenario.","title":"25) Confusion matrix: lettura e collegamento ai costi"},{"location":"quiz/#26-logit-o-random-forest-quando-scegliere-cosa","text":"Logit: confini lineari , bisogno di probabilit\u00e0 affidabili , spiegabilit\u00e0 dei coefficienti . RF: dati non-lineari , interazioni, \u201cout-of-the-box\u201d forte; poi regola soglia e, se servono probabilit\u00e0 affidabili, calibra .","title":"26) Logit o Random Forest? Quando scegliere cosa"},{"location":"quiz/#27-class_weightbalanced-cosa-fa-e-quando-usarlo","text":"Risposta: Pesa gli errori della classe minoritaria di pi\u00f9 (peso \u2248 1/frequenza). Quando: sbilanciamento marcato e interesse a recall migliore senza riscrivere la loss.","title":"27) class_weight='balanced': cosa fa e quando usarlo?"},{"location":"quiz/#28-come-controlli-la-stabilita-dei-risultati","text":"Risposta: Guarda media \u00b1 std in CV (anche ripetuta), prova pi\u00f9 seed , verifica coerenza tra OOF e test . Se la varianza \u00e8 alta, preferisci modelli/parametri pi\u00f9 semplici (regola 1-SE).","title":"28) Come controlli la stabilit\u00e0 dei risultati?"},{"location":"quiz/#29-perche-05-non-e-una-legge-per-la-soglia","text":"Risposta: 0.5 presuppone costi simmetrici e probabilit\u00e0 perfettamente calibrate. Nel mondo reale FN \u2260 FP : scegli la soglia in base a F1/Youden/costo (e dopo aver calibrato se servono probabilit\u00e0 affidabili).","title":"29) Perch\u00e9 0.5 non \u00e8 una legge per la soglia?"},{"location":"quiz/#30-workflow-che-non-ti-frega-passo-passo","text":"1) Split train/test (test in cassaforte ). 2) Pipeline con preprocess \u2192 CV \u2192 tuning (regola 1-SE se serve). 3) Soglia su validation/OOF (o per costo ). 4) Fit finale su tutto il train (stessa pipeline/parametri/soglia). 5) Valutazione sul test una sola volta. 6) Spiega : importanze/coef, curve ROC/PR, ablation, calibrazione.","title":"30) Workflow \u201cche non ti frega\u201d (passo-passo)"},{"location":"quiz/#bonus-come-leggere-un-run-summary-stampato-dagli-script","text":"Risposta: \u00c8 un estratto a prova d\u2019occhio : seed, soglia, metriche train/test , confusion e CV (media\u00b1std) , eventuale costo . Ti dice come hai scelto e quanto puoi fidarti (stabilit\u00e0, onest\u00e0 metodologica).","title":"Bonus) Come leggere un RUN SUMMARY stampato dagli script"},{"location":"quiz/#mini-glossario","text":"Parametro: numero interno al modello regolato dall\u2019algoritmo (le \u201cmanopole\u201d). Iperparametro: numero scelto dall\u2019utente con CV (profondit\u00e0, C, learning rate\u2026). Loss/Errore: numero che misura \u201cquanto sto sbagliando\u201d durante l\u2019addestramento. Generalizzazione: andare bene su dati nuovi (test). Calibrazione: rendere le probabilit\u00e0 coerenti con le frequenze reali. OOF: predizioni su train ottenute senza vedere quel campione (da altri fold). Youden: TPR\u2212FPR; criterio di scelta soglia dalla ROC. Ablation: test di robustezza rimuovendo feature importanti.","title":"Mini-glossario"},{"location":"script/","text":"Script inclusi \u00b6 Questa pagina elenca gli script del laboratorio con cosa fanno , opzioni principali e comandi tipici . Gli script stampano report \u201cparlanti\u201d (metriche, confusion, analisi soglie ecc.). Alcuni includono anche gli appunti finali con --print-cheatsheet . scripts/iris.py \u00b6 Cosa: Decision Tree sull\u2019IRIS (3 classi). Perch\u00e9: capire over/underfitting con regole leggibili (SE\u2026 ALLORA\u2026). Opzioni principali - --max-depth INT \u2026 limita la profondit\u00e0 dell\u2019albero. - --tune \u2026 sceglie in modo onesto la profondit\u00e0 via k-fold (solo sul train). - --seed INT \u2026 fissa lo split. - --print-cheatsheet \u2026 stampa appunti finali (lesson learned essenziali). Esempi python scripts/iris.py python scripts/iris.py --max-depth 3 python scripts/iris.py --tune --seed 13 --print-cheatsheet ```` **Cosa vedrai** * Accuratezza **train/test** * **Regole** dell\u2019albero in testo * **Confusion matrix** multiclasse * **Feature importance** * Mini **sweep** di profondit\u00e0 con train vs test * (se `--tune`) scelta della profondit\u00e0 via CV --- ## `scripts/imbalance.py` **Cosa:** Classificazione **sbilanciata** (Breast Cancer) con **Logistic Regression** + **scaling**. **Perch\u00e9:** mostrare che l\u2019**accuracy** pu\u00f2 ingannare; serve guardare **Precision/Recall/F1** e **scegliere la soglia**. **Opzioni principali** * `--C FLOAT` \u2026 forza del modello logit (C\u2191 = meno regolarizzazione). * `--threshold FLOAT` \u2026 soglia decisione (default 0.5). * `--auto-threshold` \u2026 sceglie la soglia da **validation** (solo train). * `--metric {f1,youden}` \u2026 criterio per l\u2019auto-soglia. * `--seed INT` * `--print-cheatsheet` \u2026 appunti finali. **Esempi** ```bash python scripts/imbalance.py python scripts/imbalance.py --auto-threshold --metric f1 --print-cheatsheet python scripts/imbalance.py --C 0.5 --seed 13 Cosa vedrai Baseline (classe maggioritaria) Distribuzione degli score (quantili per negativi/positivi) Sweep soglia (tabella soglia \u2192 FP/FN/Precision/Recall/F1) Soglia a costo (es. FN 5\u00d7 FP) Report completo: Accuracy, Precision, Recall, F1 , ROC-AUC , Confusion matrix CV ROC-AUC sul train scripts/forest_vs_logit.py \u00b6 Cosa: Confronto Random Forest vs Logistic su Breast Cancer. Include calibrazione delle probabilit\u00e0 RF e scelta soglia separata per i due modelli. Opzioni principali Soglia: --threshold FLOAT , --auto-threshold , --metric {f1,youden} Random Forest: --rf-n INT , --rf-depth {INT|None} , --rf-mf {sqrt,log2} --rf-class-weight {balanced|None} --rf-tune \u2026 grid semplice via CV (sul train) --rf-calibrate {isotonic,sigmoid} \u2026 CalibratedClassifierCV su train Logistica: --C FLOAT Generali: --seed INT , --print-cheatsheet Esempi python scripts/forest_vs_logit.py --auto-threshold python scripts/forest_vs_logit.py --rf-calibrate isotonic --auto-threshold --print-cheatsheet python scripts/forest_vs_logit.py --rf-tune --seed 13 Cosa vedrai Baseline maggioritaria Analisi punteggi RF (quantili + sweep soglia ) Report logit e RF (Accuracy, F1 , ROC-AUC , Confusion) Importanze RF (da un clone non calibrato, se necessario) CV ROC-AUC sul train per entrambi scripts/importance_demo.py \u00b6 Cosa: Importanza delle feature con Random Forest: impurity vs permutation , correlazioni e ablation (drop top-k). Perch\u00e9: non farsi ingannare dalle sole feature_importances_ . Opzioni principali --seed INT --print-cheatsheet Esempi python scripts/importance_demo.py python scripts/importance_demo.py --seed 13 --print-cheatsheet Cosa vedrai AUC e Accuracy base della RF Impurity importance (top-k) Permutation importance su TEST (metrica ROC-AUC) Coppie molto correlate (|corr| \u2265 0.9) \u2014 warning interpretativo Ablation : AUC base vs AUC senza top-1 / top-3 scripts/gridsearch_mixed.py \u00b6 Cosa: Dati misti (numeriche + categoriche, con missing) con Pipeline/ColumnTransformer e GridSearchCV (no leakage). Sceglie la soglia onesta da OOF (predizioni out-of-fold). Supporta Logit e RF . Opzioni principali (le pi\u00f9 usate) Modello: --model {logit,rf} Soglia: --auto-threshold , --thr-mode {f1,youden,cost} , --cost-fp FLOAT , --cost-fn FLOAT Dati: --missing FLOAT (quota di valori mancanti generati), --seed INT (Altre opzioni sono mostrate da --help ) Esempi # Logit con soglia OOF (F1) python scripts/gridsearch_mixed.py --model logit --auto-threshold # RF con soglia a costo (FN 10\u00d7 FP) python scripts/gridsearch_mixed.py --model rf --auto-threshold --thr-mode cost --cost-fn 10 Cosa vedrai Top configurazioni da GridSearch (per F1 CV) + AUC Soglia OOF scelta (criterio F1/Youden/costo) Report TRAIN e TEST alla soglia scelta Per Logit : Top coefficienti (segnano +/\u2212 rischio) Per RF : Top importanze Cosa vedrai Confronto affiancato: Accuracy e F1 CV (train) con media \u00b1 std per la versione corretta Convenzioni di output (rapido promemoria) \u00b6 Metriche chiave: Accuracy, Precision , Recall , F1 , ROC-AUC Confusion matrix con conteggi TN/FP/FN/TP \u201cin chiaro\u201d Soglia: tabella \u201csweep soglia\u201d e/o scelta auto (validation/OOF) o a costo CV: riportata come media \u00b1 deviazione standard (k-fold sul train ) Tip: se perdi pochi positivi \u00e8 grave, abbassa la soglia; se odi i falsi allarmi , alzala. Con RF valuta calibrazione ( isotonic / sigmoid ) quando servono probabilit\u00e0 affidabili. Turbo-K \u2014 valutatore di bucket (CIDR, \u03c7\u00b2/DoF) + search/compare oli \u00b6 Script: scripts/turbo_k_eval.py A cosa serve: verifica quanto \u00e8 uniforme l\u2019assegnazione a bucket di una permutazione affine su IPv4: y = (a*x + b) mod 2^32 , con bucket da MSB ( K bit alti) o mod M . Stampa \u03c7\u00b2/DoF , bucket top/bottom e offre ricerca/sorteggio di oli: --search-a , --search-b , confronto --compare , preset --presets . Esempi veloci # Uniforme, MSB K=12 python scripts/turbo_k_eval.py --source uniform --mode msb --K 12 --N 200000 # CIDR mix, suggerisci 'a' migliore (128 tentativi) python scripts/turbo_k_eval.py --source cidr \\ --cidr 10.0.0.0/8 --cidr 192.168.0.0/16 \\ --mode msb --K 12 --N 300000 --search-a 128 # Confronto oli (coppie a,b) python scripts/turbo_k_eval.py --K 12 \\ --compare 0xDEADBEEF,0xBADC0FFE \\ --compare 0xA5A5A5A5,0x1 # Preset oli da file python scripts/turbo_k_eval.py --presets presets/oils.yaml \\ --source uniform --mode msb --K 12 --N 200000 Opzioni utili --mode {msb,mod} + --K (MSB) o --M (mod). --search-a N / --search-b N (con --min-popcount-b per evitare b troppo poveri di bit alti). --compare a,b (ripetibile). --presets presets/oils.yaml (campione di oli nominati). --print-cheatsheet (promemoria a fine run). Interpretazione rapida Target \u03c7\u00b2/DoF \u2248 1 (verde ~ 0.9\u20131.2). Se E = N/B < 50 aspettati varianza alta \u201cfisiologica\u201d. Se sorgente = file con molte ripetizioni, usa il modo normalizzato stampato dal tool: guarda \u201c\u03c7\u00b2/DoF (solo unici)\u201d e \u201c\u03c7\u00b2/DoF normalizzato\u201d (~1 se il mixing \u00e8 sano).","title":"Script"},{"location":"script/#script-inclusi","text":"Questa pagina elenca gli script del laboratorio con cosa fanno , opzioni principali e comandi tipici . Gli script stampano report \u201cparlanti\u201d (metriche, confusion, analisi soglie ecc.). Alcuni includono anche gli appunti finali con --print-cheatsheet .","title":"Script inclusi"},{"location":"script/#scriptsirispy","text":"Cosa: Decision Tree sull\u2019IRIS (3 classi). Perch\u00e9: capire over/underfitting con regole leggibili (SE\u2026 ALLORA\u2026). Opzioni principali - --max-depth INT \u2026 limita la profondit\u00e0 dell\u2019albero. - --tune \u2026 sceglie in modo onesto la profondit\u00e0 via k-fold (solo sul train). - --seed INT \u2026 fissa lo split. - --print-cheatsheet \u2026 stampa appunti finali (lesson learned essenziali). Esempi python scripts/iris.py python scripts/iris.py --max-depth 3 python scripts/iris.py --tune --seed 13 --print-cheatsheet ```` **Cosa vedrai** * Accuratezza **train/test** * **Regole** dell\u2019albero in testo * **Confusion matrix** multiclasse * **Feature importance** * Mini **sweep** di profondit\u00e0 con train vs test * (se `--tune`) scelta della profondit\u00e0 via CV --- ## `scripts/imbalance.py` **Cosa:** Classificazione **sbilanciata** (Breast Cancer) con **Logistic Regression** + **scaling**. **Perch\u00e9:** mostrare che l\u2019**accuracy** pu\u00f2 ingannare; serve guardare **Precision/Recall/F1** e **scegliere la soglia**. **Opzioni principali** * `--C FLOAT` \u2026 forza del modello logit (C\u2191 = meno regolarizzazione). * `--threshold FLOAT` \u2026 soglia decisione (default 0.5). * `--auto-threshold` \u2026 sceglie la soglia da **validation** (solo train). * `--metric {f1,youden}` \u2026 criterio per l\u2019auto-soglia. * `--seed INT` * `--print-cheatsheet` \u2026 appunti finali. **Esempi** ```bash python scripts/imbalance.py python scripts/imbalance.py --auto-threshold --metric f1 --print-cheatsheet python scripts/imbalance.py --C 0.5 --seed 13 Cosa vedrai Baseline (classe maggioritaria) Distribuzione degli score (quantili per negativi/positivi) Sweep soglia (tabella soglia \u2192 FP/FN/Precision/Recall/F1) Soglia a costo (es. FN 5\u00d7 FP) Report completo: Accuracy, Precision, Recall, F1 , ROC-AUC , Confusion matrix CV ROC-AUC sul train","title":"scripts/iris.py"},{"location":"script/#scriptsforest_vs_logitpy","text":"Cosa: Confronto Random Forest vs Logistic su Breast Cancer. Include calibrazione delle probabilit\u00e0 RF e scelta soglia separata per i due modelli. Opzioni principali Soglia: --threshold FLOAT , --auto-threshold , --metric {f1,youden} Random Forest: --rf-n INT , --rf-depth {INT|None} , --rf-mf {sqrt,log2} --rf-class-weight {balanced|None} --rf-tune \u2026 grid semplice via CV (sul train) --rf-calibrate {isotonic,sigmoid} \u2026 CalibratedClassifierCV su train Logistica: --C FLOAT Generali: --seed INT , --print-cheatsheet Esempi python scripts/forest_vs_logit.py --auto-threshold python scripts/forest_vs_logit.py --rf-calibrate isotonic --auto-threshold --print-cheatsheet python scripts/forest_vs_logit.py --rf-tune --seed 13 Cosa vedrai Baseline maggioritaria Analisi punteggi RF (quantili + sweep soglia ) Report logit e RF (Accuracy, F1 , ROC-AUC , Confusion) Importanze RF (da un clone non calibrato, se necessario) CV ROC-AUC sul train per entrambi","title":"scripts/forest_vs_logit.py"},{"location":"script/#scriptsimportance_demopy","text":"Cosa: Importanza delle feature con Random Forest: impurity vs permutation , correlazioni e ablation (drop top-k). Perch\u00e9: non farsi ingannare dalle sole feature_importances_ . Opzioni principali --seed INT --print-cheatsheet Esempi python scripts/importance_demo.py python scripts/importance_demo.py --seed 13 --print-cheatsheet Cosa vedrai AUC e Accuracy base della RF Impurity importance (top-k) Permutation importance su TEST (metrica ROC-AUC) Coppie molto correlate (|corr| \u2265 0.9) \u2014 warning interpretativo Ablation : AUC base vs AUC senza top-1 / top-3","title":"scripts/importance_demo.py"},{"location":"script/#scriptsgridsearch_mixedpy","text":"Cosa: Dati misti (numeriche + categoriche, con missing) con Pipeline/ColumnTransformer e GridSearchCV (no leakage). Sceglie la soglia onesta da OOF (predizioni out-of-fold). Supporta Logit e RF . Opzioni principali (le pi\u00f9 usate) Modello: --model {logit,rf} Soglia: --auto-threshold , --thr-mode {f1,youden,cost} , --cost-fp FLOAT , --cost-fn FLOAT Dati: --missing FLOAT (quota di valori mancanti generati), --seed INT (Altre opzioni sono mostrate da --help ) Esempi # Logit con soglia OOF (F1) python scripts/gridsearch_mixed.py --model logit --auto-threshold # RF con soglia a costo (FN 10\u00d7 FP) python scripts/gridsearch_mixed.py --model rf --auto-threshold --thr-mode cost --cost-fn 10 Cosa vedrai Top configurazioni da GridSearch (per F1 CV) + AUC Soglia OOF scelta (criterio F1/Youden/costo) Report TRAIN e TEST alla soglia scelta Per Logit : Top coefficienti (segnano +/\u2212 rischio) Per RF : Top importanze Cosa vedrai Confronto affiancato: Accuracy e F1 CV (train) con media \u00b1 std per la versione corretta","title":"scripts/gridsearch_mixed.py"},{"location":"script/#convenzioni-di-output-rapido-promemoria","text":"Metriche chiave: Accuracy, Precision , Recall , F1 , ROC-AUC Confusion matrix con conteggi TN/FP/FN/TP \u201cin chiaro\u201d Soglia: tabella \u201csweep soglia\u201d e/o scelta auto (validation/OOF) o a costo CV: riportata come media \u00b1 deviazione standard (k-fold sul train ) Tip: se perdi pochi positivi \u00e8 grave, abbassa la soglia; se odi i falsi allarmi , alzala. Con RF valuta calibrazione ( isotonic / sigmoid ) quando servono probabilit\u00e0 affidabili.","title":"Convenzioni di output (rapido promemoria)"},{"location":"script/#turbo-k-valutatore-di-bucket-cidr-2dof-searchcompare-oli","text":"Script: scripts/turbo_k_eval.py A cosa serve: verifica quanto \u00e8 uniforme l\u2019assegnazione a bucket di una permutazione affine su IPv4: y = (a*x + b) mod 2^32 , con bucket da MSB ( K bit alti) o mod M . Stampa \u03c7\u00b2/DoF , bucket top/bottom e offre ricerca/sorteggio di oli: --search-a , --search-b , confronto --compare , preset --presets . Esempi veloci # Uniforme, MSB K=12 python scripts/turbo_k_eval.py --source uniform --mode msb --K 12 --N 200000 # CIDR mix, suggerisci 'a' migliore (128 tentativi) python scripts/turbo_k_eval.py --source cidr \\ --cidr 10.0.0.0/8 --cidr 192.168.0.0/16 \\ --mode msb --K 12 --N 300000 --search-a 128 # Confronto oli (coppie a,b) python scripts/turbo_k_eval.py --K 12 \\ --compare 0xDEADBEEF,0xBADC0FFE \\ --compare 0xA5A5A5A5,0x1 # Preset oli da file python scripts/turbo_k_eval.py --presets presets/oils.yaml \\ --source uniform --mode msb --K 12 --N 200000 Opzioni utili --mode {msb,mod} + --K (MSB) o --M (mod). --search-a N / --search-b N (con --min-popcount-b per evitare b troppo poveri di bit alti). --compare a,b (ripetibile). --presets presets/oils.yaml (campione di oli nominati). --print-cheatsheet (promemoria a fine run). Interpretazione rapida Target \u03c7\u00b2/DoF \u2248 1 (verde ~ 0.9\u20131.2). Se E = N/B < 50 aspettati varianza alta \u201cfisiologica\u201d. Se sorgente = file con molte ripetizioni, usa il modo normalizzato stampato dal tool: guarda \u201c\u03c7\u00b2/DoF (solo unici)\u201d e \u201c\u03c7\u00b2/DoF normalizzato\u201d (~1 se il mixing \u00e8 sano).","title":"Turbo-K \u2014 valutatore di bucket (CIDR, \u03c7\u00b2/DoF) + search/compare oli"}]}