<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="../img/favicon.ico">
    <title>Quiz &mdash; ML-Lab</title>
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/tonsky/FiraCode@1.206/distr/fira_code.css">
    <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.8.1/css/all.css">
    <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.8.1/css/v4-shims.css">
    <link rel="stylesheet" href="../css/theme.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    <script src="//code.jquery.com/jquery-2.1.1.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>
        hljs.initHighlightingOnLoad();
    </script> 
</head>

<body ontouchstart="">
    <div id="container">
        <aside>
            <div class="home">
                <div class="title">
                    <button class="hamburger"></button>
                    <a href=".." class="site-name"> ML-Lab</a>
                </div>
                <div class="search">
                    <div role="search">
    <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
        <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
    </form>
</div>
                </div>
            </div>
            <nav class="nav">
                <ul class="root">
                    <li class="toctree-l1"><a class="nav-item" href="..">Home</a></li>
                    <li class="toctree-l1"><a class="nav-item" href="../lessons-learned/">Lessons Learned</a></li>
                    <li class="toctree-l1"><a class="nav-item" href="../script/">Script</a></li>
                    <li class="toctree-l1 current"><a class="nav-item current" href="./">Quiz</a>
<ul class="subnav">
<li class="toctree-l2"><a class="nav-item toc" href="#1-che-cose-in-una-frase-il-machine-learning">1) Che cos’è, in una frase, il Machine Learning?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#2-perche-il-test-e-sacro">2) Perché il “test è sacro”?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#3-overfitting-vs-underfitting-differenza-pratica-e-rimedi">3) Overfitting vs Underfitting: differenza pratica e rimedi</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#4-decision-tree-perche-e-spiegabile-e-come-ne-controlli-la-complessita">4) Decision Tree: perché è spiegabile e come ne controlli la complessità?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#5-perche-una-random-forest-riduce-la-varianza-dellalbero-singolo">5) Perché una Random Forest riduce la varianza dell’albero singolo?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#6-cose-un-iperparametro-esempi">6) Cos’è un iperparametro? Esempi</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#7-k-fold-cross-validation-cose-e-perche-sul-solo-train">7) k-fold Cross-Validation: cos’è e perché sul solo train?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#8-regola-1-se-quando-e-perche">8) Regola 1-SE: quando e perché?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#9-perche-laccuracy-puo-mentire-con-classi-sbilanciate">9) Perché l’accuracy può mentire con classi sbilanciate?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#10-precision-recall-f1-definizioni-pratiche">10) Precision, Recall, F1: definizioni pratiche</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#11-roc-auc-cosa-misura-e-perche-non-dipende-da-05">11) ROC-AUC: cosa misura e perché non dipende da 0.5</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#12-come-si-sceglie-la-soglia-tre-strade">12) Come si sceglie la soglia? Tre strade</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#13-cosa-sono-youden-e-oof">13) Cosa sono Youden e OOF?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#14-quando-serve-lo-scaling">14) Quando serve lo scaling?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#15-logistic-regression-procontro-e-iper-chiave">15) Logistic Regression: pro/contro e iper chiave</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#16-random-forest-procontro-e-pomelli">16) Random Forest: pro/contro e pomelli</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#17-perche-calibrare-le-probabilita-di-una-rf-come">17) Perché calibrare le probabilità di una RF? Come?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#18-importanza-delle-feature-impurity-vs-permutation">18) Importanza delle feature: “impurity” vs “permutation”</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#19-effetto-delle-feature-molto-correlate">19) Effetto delle feature molto correlate</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#20-cose-lablation-test">20) Cos’è l’ablation test?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#21-cose-il-data-leakage-esempio-e-prevenzione">21) Cos’è il data leakage? Esempio e prevenzione</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#22-perche-pipeline-gridsearchcv-previene-il-leakage">22) Perché Pipeline + GridSearchCV previene il leakage?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#23-predizioni-oof-perche-usarle-per-la-soglia">23) Predizioni OOF: perché usarle per la soglia?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#24-refit-su-f1-cosa-vuol-dire-e-come-valuti-il-test">24) “Refit su F1”: cosa vuol dire e come valuti il test?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#25-confusion-matrix-lettura-e-collegamento-ai-costi">25) Confusion matrix: lettura e collegamento ai costi</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#26-logit-o-random-forest-quando-scegliere-cosa">26) Logit o Random Forest? Quando scegliere cosa</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#27-class_weightbalanced-cosa-fa-e-quando-usarlo">27) class_weight='balanced': cosa fa e quando usarlo?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#28-come-controlli-la-stabilita-dei-risultati">28) Come controlli la stabilità dei risultati?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#29-perche-05-non-e-una-legge-per-la-soglia">29) Perché 0.5 non è una legge per la soglia?</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#30-workflow-che-non-ti-frega-passo-passo">30) Workflow “che non ti frega” (passo-passo)</a></li>
<li class="toctree-l2"><a class="nav-item toc" href="#bonus-come-leggere-un-run-summary-stampato-dagli-script">Bonus) Come leggere un RUN SUMMARY stampato dagli script</a></li>
</ul></li>
                </ul>
            </nav>
            <div class="repo">
    <div class="link">
    </div>
    <div class="previous"><a href="../script/">&laquo; Previous</a></div>
</div>
        </aside>
        <div id="spacer"><button class="arrow"></button></div>
        <main>
            <div class="home-top">
                <button class="hamburger"></button>
                <a href=".." class="site-name"> ML-Lab</a>
            </div>
            <div id="main">
                <nav class="breadcrumbs">
<ul>
    
</ul>
</nav>
                <div id="content"><h1 id="ml-lab-quiz-qa-versione-chiara">ML-Lab — Quiz Q&amp;A (versione chiara)<a class="headerlink" href="#ml-lab-quiz-qa-versione-chiara" title="Permanent link">&para;</a></h1>
<p><em>Studio autonomo — domande &amp; risposte semplici ma dettagliate.</em></p>
<blockquote>
<p>Consiglio: prova prima a rispondere da solo, poi confronta con la soluzione.
Ogni risposta include il “perché” e spesso un mini-esempio pratico.</p>
</blockquote>
<hr />
<h2 id="1-che-cose-in-una-frase-il-machine-learning">1) Che cos’è, in una frase, il Machine Learning?<a class="headerlink" href="#1-che-cose-in-una-frase-il-machine-learning" title="Permanent link">&para;</a></h2>
<p><strong>Risposta (umana):</strong> Costruire un programma/funzione che <strong>impara una determinata regola dai dati</strong>: gli mostri esempi di <strong>input</strong> e il <strong>risultato giusto</strong>, e lui <strong>regola da solo</strong> le sue manopole per <strong>sbagliare il meno possibile</strong> sugli esempi, puntando ad andare bene anche su <strong>dati nuovi</strong>.
- <strong>Parametri = manopole</strong> del modello (numeri).<br />
- <strong>Algoritmo di apprendimento</strong> = come giriamo le manopole per ridurre l’<strong>errore</strong>.<br />
- <strong>Generalizzazione</strong> = andare bene su <strong>test</strong> nuovi, su dati mai visti.</p>
<hr />
<h2 id="2-perche-il-test-e-sacro">2) Perché il “test è sacro”?<a class="headerlink" href="#2-perche-il-test-e-sacro" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Serve a misurare la <strong>generalizzazione vera</strong>. Se lo usi per scegliere iperparametri (o le soglie), <strong>inquini</strong> la misura (stai “imparando” anche dal test ed è scorretto).<br />
<strong>Regola:</strong> tutte le decisioni (tuning, soglia, trasformazioni) vanno prese <strong>solo</strong> col <strong>train</strong> (CV/validation). Il test si usa <strong>una volta alla fine</strong>.</p>
<hr />
<h2 id="3-overfitting-vs-underfitting-differenza-pratica-e-rimedi">3) Overfitting vs Underfitting: differenza pratica e rimedi<a class="headerlink" href="#3-overfitting-vs-underfitting-differenza-pratica-e-rimedi" title="Permanent link">&para;</a></h2>
<p><strong>Overfit:</strong> ottimo su <strong>train</strong>, peggio su <strong>test</strong> → troppo complesso / ha “pappagallato”.<br />
<strong>Underfit:</strong> scarso su <strong>train</strong> e <strong>test</strong> → troppo semplice / info insufficienti.<br />
<strong>Rimedi:</strong> riduci o aumenti la complessità (es. <code>max_depth</code>, <code>C</code>), aggiungi dati/feature, regolarizzazione, semplifica il modello.</p>
<hr />
<h2 id="4-decision-tree-perche-e-spiegabile-e-come-ne-controlli-la-complessita">4) Decision Tree: perché è spiegabile e come ne controlli la complessità?<a class="headerlink" href="#4-decision-tree-perche-e-spiegabile-e-come-ne-controlli-la-complessita" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> L’albero produce regole <strong>SE … ALLORA …</strong> leggibili (threshold sulle feature).<br />
<strong>Controllo:</strong> <code>max_depth</code>, <code>min_samples_leaf</code>, <code>max_leaf_nodes</code>.<br />
<strong>Nota:</strong> un albero singolo ha <strong>alta varianza</strong> → piccole variazioni nei dati cambiano molto la struttura.</p>
<hr />
<h2 id="5-perche-una-random-forest-riduce-la-varianza-dellalbero-singolo">5) Perché una Random Forest riduce la varianza dell’albero singolo?<a class="headerlink" href="#5-perche-una-random-forest-riduce-la-varianza-dellalbero-singolo" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Fa la <strong>media</strong> di molti alberi addestrati su <strong>campioni diversi</strong> e <strong>sottoinsiemi di feature</strong> (bagging + casualità). La media <strong>stabilizza</strong> e riduce errori “capricciosi”.</p>
<hr />
<h2 id="6-cose-un-iperparametro-esempi">6) Cos’è un iperparametro? Esempi<a class="headerlink" href="#6-cose-un-iperparametro-esempi" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> È un <strong>controllo del modello</strong> che <strong>non</strong> viene imparato dai dati ma <strong>scelto</strong> dall’utente con CV.<br />
<strong>Esempi:</strong> <code>max_depth</code> (Tree/RF), <code>n_estimators</code> (RF), <code>C</code> (Logit/SVM), <code>max_features</code> (RF), <code>learning_rate</code> (boosting).</p>
<hr />
<h2 id="7-k-fold-cross-validation-cose-e-perche-sul-solo-train">7) k-fold Cross-Validation: cos’è e perché sul solo train?<a class="headerlink" href="#7-k-fold-cross-validation-cose-e-perche-sul-solo-train" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Dividi il <strong>train</strong> in k parti; ripeti k volte: alleni su k-1 parti, valuti sulla parte rimasta. Media e deviazione standard ti danno una <strong>stima stabile</strong>. Si fa solo sul <strong>train</strong> per non “guardare” il <strong>test</strong>.</p>
<hr />
<h2 id="8-regola-1-se-quando-e-perche">8) Regola 1-SE: quando e perché?<a class="headerlink" href="#8-regola-1-se-quando-e-perche" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Se più modelli sono <strong>statisticamente equivalenti</strong> (entro 1 deviazione standard dalla migliore metrica), scegli la <strong>versione più semplice</strong>. È più <strong>robusta</strong> su dati nuovi.</p>
<hr />
<h2 id="9-perche-laccuracy-puo-mentire-con-classi-sbilanciate">9) Perché l’accuracy può mentire con classi sbilanciate?<a class="headerlink" href="#9-perche-laccuracy-puo-mentire-con-classi-sbilanciate" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Se i positivi sono rari, dire “sempre negativo” può dare alta accuracy ma <strong>recall=0</strong>.<br />
<strong>Soluzione:</strong> guarda <strong>Precision</strong>, <strong>Recall</strong>, <strong>F1</strong> e la <strong>Confusion matrix</strong>.</p>
<hr />
<h2 id="10-precision-recall-f1-definizioni-pratiche">10) Precision, Recall, F1: definizioni pratiche<a class="headerlink" href="#10-precision-recall-f1-definizioni-pratiche" title="Permanent link">&para;</a></h2>
<p><strong>Precision</strong> = tra gli <strong>allarmi</strong> che hai dato, quanti erano <strong>giusti</strong>? = <code>TP/(TP+FP)</code><br />
<strong>Recall</strong> = tra i <strong>positivi veri</strong>, quanti <strong>non hai perso</strong>? = <code>TP/(TP+FN)</code><br />
<strong>F1</strong> = media armonica di Precision e Recall → sale <strong>solo</strong> se <strong>entrambi</strong> sono alti.<br />
<strong>Quando usarle:</strong> problemi sbilanciati o quando i costi di FP/FN sono diversi.</p>
<hr />
<h2 id="11-roc-auc-cosa-misura-e-perche-non-dipende-da-05">11) ROC-AUC: cosa misura e perché non dipende da 0.5<a class="headerlink" href="#11-roc-auc-cosa-misura-e-perche-non-dipende-da-05" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Valuta la qualità del <strong>ranking</strong> delle probabilità su <strong>tutte le soglie</strong>.<br />
<strong>Interpretazione:</strong> probabilità che un positivo abbia score &gt; di un negativo. <strong>0.5</strong> ≈ a caso, <strong>1.0</strong> perfetto. Non usa la soglia fissa 0.5.</p>
<hr />
<h2 id="12-come-si-sceglie-la-soglia-tre-strade">12) Come si sceglie la soglia? Tre strade<a class="headerlink" href="#12-come-si-sceglie-la-soglia-tre-strade" title="Permanent link">&para;</a></h2>
<p>1) <strong>Auto-threshold</strong> su validation/OOF per massimizzare <strong>F1</strong> o <strong>Youden = TPR−FPR</strong>.<br />
2) <strong>A costo</strong>: minimizza <code>c_fp·FP + c_fn·FN</code> (decidi tu quanto pesa perdere un positivo).<br />
3) <strong>Sweep</strong>: stampi la tabella soglia → (FP, FN, Precision, Recall, F1) e scegli consapevolmente.<br />
<strong>Regola pratica:</strong> se <strong>FN è gravissimo</strong>, abbassa la soglia.</p>
<hr />
<h2 id="13-cosa-sono-youden-e-oof">13) Cosa sono Youden e OOF?<a class="headerlink" href="#13-cosa-sono-youden-e-oof" title="Permanent link">&para;</a></h2>
<p><strong>Youden J:</strong> <code>TPR − FPR</code> → cerca la soglia più distante dalla diagonale della ROC.<br />
<strong>OOF (Out-Of-Fold):</strong> predizioni di CV sul <strong>train</strong>, fatte da modelli che <strong>non</strong> hanno visto quel campione. Servono per scelte <strong>oneste</strong> (soglia, calibrazione) senza toccare il test.</p>
<hr />
<h2 id="14-quando-serve-lo-scaling">14) Quando serve lo scaling?<a class="headerlink" href="#14-quando-serve-lo-scaling" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Serve per modelli che usano <strong>distanze o geometria</strong> (Logit, SVM, k-NN, PCA).<br />
In genere <strong>non serve</strong> per <strong>Tree/Random Forest</strong> (lavorano per soglie, non distanze).</p>
<hr />
<h2 id="15-logistic-regression-procontro-e-iper-chiave">15) Logistic Regression: pro/contro e iper chiave<a class="headerlink" href="#15-logistic-regression-procontro-e-iper-chiave" title="Permanent link">&para;</a></h2>
<p><strong>Pro:</strong> semplice, veloce, <strong>probabilità ben calibrate</strong>, ottima su confini <strong>quasi lineari</strong>.<br />
<strong>Contro:</strong> fatica con <strong>forte non-linearità</strong> se non aggiungi feature/interazioni.<br />
<strong>Iper:</strong> <code>C</code> (meno regolarizzazione se alto) → occhio all’overfit; <strong>scalare</strong> le feature è essenziale.</p>
<hr />
<h2 id="16-random-forest-procontro-e-pomelli">16) Random Forest: pro/contro e pomelli<a class="headerlink" href="#16-random-forest-procontro-e-pomelli" title="Permanent link">&para;</a></h2>
<p><strong>Pro:</strong> coglie <strong>non-linearità</strong> e <strong>interazioni</strong>, robusta, pochi settaggi critici.<br />
<strong>Contro:</strong> <strong>probabilità</strong> spesso <strong>non calibrate</strong>; spiegabilità globale limitata.<br />
<strong>Pomelli:</strong> <code>n_estimators</code>, <code>max_depth</code>, <code>max_features</code>, <code>class_weight</code> per sbilanciamento.</p>
<hr />
<h2 id="17-perche-calibrare-le-probabilita-di-una-rf-come">17) Perché calibrare le probabilità di una RF? Come?<a class="headerlink" href="#17-perche-calibrare-le-probabilita-di-una-rf-come" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Perché gli score possono essere <strong>troppo ottimisti/piatti</strong>. Con <code>CalibratedClassifierCV</code>:
- <strong>sigmoid (Platt):</strong> semplice, pochi dati.<br />
- <strong>isotonic:</strong> più flessibile, richiede più dati.<br />
<strong>Trucco pratico:</strong> per leggere <code>feature_importances_</code> quando usi la calibrazione, rifitta un <strong>clone</strong> della RF <strong>non calibrata</strong> sul train.</p>
<hr />
<h2 id="18-importanza-delle-feature-impurity-vs-permutation">18) Importanza delle feature: “impurity” vs “permutation”<a class="headerlink" href="#18-importanza-delle-feature-impurity-vs-permutation" title="Permanent link">&para;</a></h2>
<p><strong>Impurity (feature_importances_)</strong>: veloce, ma <strong>favorisce</strong> feature con tante soglie; con feature <strong>correlate</strong> si <strong>divide il merito</strong>.<br />
<strong>Permutation (su TEST)</strong>: misura il <strong>calo reale</strong> di metrica quando “rompi” la feature → spesso <strong>più onesta</strong>.<br />
<strong>Uso consigliato:</strong> guarda <strong>entrambe</strong> e conferma con <strong>ablation</strong>.</p>
<hr />
<h2 id="19-effetto-delle-feature-molto-correlate">19) Effetto delle feature molto correlate<a class="headerlink" href="#19-effetto-delle-feature-molto-correlate" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Se due feature dicono <strong>la stessa cosa</strong>, l’importanza si <strong>spalma</strong> su entrambe: nessuna sembra “dominare” da sola.<br />
<strong>Cosa fare:</strong> controlla <strong>correlazioni</strong> (|ρ|≥0.9), prova <strong>ablation</strong>, valuta <strong>grouping</strong>/RIDUZIONE (PCA o scelta di una sola).</p>
<hr />
<h2 id="20-cose-lablation-test">20) Cos’è l’ablation test?<a class="headerlink" href="#20-cose-lablation-test" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Togli le top-1/top-k feature e <strong>misuri</strong> quanto <strong>scende</strong> la metrica (es. AUC).<br />
<strong>Lettura:</strong> se non scende, il segnale era <strong>ridondante</strong>; se crolla, quelle feature sono <strong>cruciali</strong>.</p>
<hr />
<h2 id="21-cose-il-data-leakage-esempio-e-prevenzione">21) Cos’è il data leakage? Esempio e prevenzione<a class="headerlink" href="#21-cose-il-data-leakage-esempio-e-prevenzione" title="Permanent link">&para;</a></h2>
<p><strong>Leakage:</strong> usi info del <strong>futuro/test</strong> nell’addestramento.<br />
<strong>Esempio:</strong> scalare o imputare sull’<strong>intero</strong> dataset prima dello split/CV.<br />
<strong>Prevenzione:</strong> tutte le trasformazioni in <strong>Pipeline/ColumnTransformer</strong>, così ogni fold fitta <strong>solo sul proprio train</strong>.</p>
<hr />
<h2 id="22-perche-pipeline-gridsearchcv-previene-il-leakage">22) Perché Pipeline + GridSearchCV previene il leakage?<a class="headerlink" href="#22-perche-pipeline-gridsearchcv-previene-il-leakage" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Perché imputazione/scaling/encoding e modello sono <strong>insieme</strong> nella pipeline e vengono <strong>fittati dentro la CV</strong> solo sui dati di <strong>train</strong> del fold. Il fold di validazione (e il test finale) restano <strong>puliti</strong>.</p>
<hr />
<h2 id="23-predizioni-oof-perche-usarle-per-la-soglia">23) Predizioni OOF: perché usarle per la soglia?<a class="headerlink" href="#23-predizioni-oof-perche-usarle-per-la-soglia" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Sono predizioni su campioni mai visti dal rispettivo modello di fold → sono <strong>oneste</strong> come un mini-test interno. Permettono di scegliere soglie/calibrazioni <strong>senza toccare</strong> il test.</p>
<hr />
<h2 id="24-refit-su-f1-cosa-vuol-dire-e-come-valuti-il-test">24) “Refit su F1”: cosa vuol dire e come valuti il test?<a class="headerlink" href="#24-refit-su-f1-cosa-vuol-dire-e-come-valuti-il-test" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> In <code>GridSearchCV(refit="f1")</code>, dopo la CV si ri-addestra il <strong>miglior modello</strong> (per F1) su <strong>tutto il train</strong>. Poi si valuta <strong>una sola volta</strong> sul <strong>test</strong> (niente ulteriori scelte sul test).</p>
<hr />
<h2 id="25-confusion-matrix-lettura-e-collegamento-ai-costi">25) Confusion matrix: lettura e collegamento ai costi<a class="headerlink" href="#25-confusion-matrix-lettura-e-collegamento-ai-costi" title="Permanent link">&para;</a></h2>
<p><strong>TN:</strong> negativi corretti; <strong>FP:</strong> falsi allarmi; <strong>FN:</strong> positivi persi; <strong>TP:</strong> positivi presi.<br />
<strong>Costo totale:</strong> <code>c_fp·FP + c_fn·FN</code>. <strong>Soglia a costo:</strong> scegli la soglia che <strong>minimizza</strong> questo valore, in base al tuo scenario.</p>
<hr />
<h2 id="26-logit-o-random-forest-quando-scegliere-cosa">26) Logit o Random Forest? Quando scegliere cosa<a class="headerlink" href="#26-logit-o-random-forest-quando-scegliere-cosa" title="Permanent link">&para;</a></h2>
<p><strong>Logit:</strong> confini <strong>lineari</strong>, bisogno di <strong>probabilità affidabili</strong>, spiegabilità dei <strong>coefficienti</strong>.<br />
<strong>RF:</strong> dati <strong>non-lineari</strong>, interazioni, “out-of-the-box” forte; poi regola <strong>soglia</strong> e, se servono probabilità affidabili, <strong>calibra</strong>.</p>
<hr />
<h2 id="27-class_weightbalanced-cosa-fa-e-quando-usarlo">27) <code>class_weight='balanced'</code>: cosa fa e quando usarlo?<a class="headerlink" href="#27-class_weightbalanced-cosa-fa-e-quando-usarlo" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Pesa gli errori della classe <strong>minoritaria</strong> di più (peso ≈ 1/frequenza).<br />
<strong>Quando:</strong> sbilanciamento marcato e interesse a <strong>recall</strong> migliore senza riscrivere la loss.</p>
<hr />
<h2 id="28-come-controlli-la-stabilita-dei-risultati">28) Come controlli la stabilità dei risultati?<a class="headerlink" href="#28-come-controlli-la-stabilita-dei-risultati" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> Guarda <strong>media ± std</strong> in <strong>CV</strong> (anche ripetuta), prova più <strong>seed</strong>, verifica coerenza tra <strong>OOF</strong> e <strong>test</strong>. Se la varianza è alta, preferisci modelli/parametri più <strong>semplici</strong> (regola 1-SE).</p>
<hr />
<h2 id="29-perche-05-non-e-una-legge-per-la-soglia">29) Perché 0.5 non è una legge per la soglia?<a class="headerlink" href="#29-perche-05-non-e-una-legge-per-la-soglia" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> 0.5 presuppone <strong>costi simmetrici</strong> e probabilità perfettamente calibrate. Nel mondo reale <strong>FN ≠ FP</strong>: scegli la soglia in base a <strong>F1/Youden/costo</strong> (e dopo aver <strong>calibrato</strong> se servono probabilità affidabili).</p>
<hr />
<h2 id="30-workflow-che-non-ti-frega-passo-passo">30) Workflow “che non ti frega” (passo-passo)<a class="headerlink" href="#30-workflow-che-non-ti-frega-passo-passo" title="Permanent link">&para;</a></h2>
<p>1) Split train/test (test <strong>in cassaforte</strong>).<br />
2) <strong>Pipeline</strong> con preprocess → <strong>CV</strong> → <strong>tuning</strong> (regola 1-SE se serve).<br />
3) <strong>Soglia</strong> su validation/OOF (o per <strong>costo</strong>).<br />
4) <strong>Fit finale</strong> su tutto il train (stessa pipeline/parametri/soglia).<br />
5) <strong>Valutazione</strong> sul test una sola volta.<br />
6) <strong>Spiega</strong>: importanze/coef, curve ROC/PR, ablation, calibrazione.</p>
<hr />
<h2 id="bonus-come-leggere-un-run-summary-stampato-dagli-script">Bonus) Come leggere un RUN SUMMARY stampato dagli script<a class="headerlink" href="#bonus-come-leggere-un-run-summary-stampato-dagli-script" title="Permanent link">&para;</a></h2>
<p><strong>Risposta:</strong> È un estratto <strong>a prova d’occhio</strong>: seed, soglia, metriche <strong>train/test</strong>, <strong>confusion</strong> e <strong>CV (media±std)</strong>, eventuale <strong>costo</strong>. Ti dice <strong>come</strong> hai scelto e <strong>quanto</strong> puoi fidarti (stabilità, onestà metodologica).</p>
<hr />
<h3 id="mini-glossario">Mini-glossario<a class="headerlink" href="#mini-glossario" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Parametro:</strong> numero interno al modello regolato dall’algoritmo (le “manopole”).  </li>
<li><strong>Iperparametro:</strong> numero scelto dall’utente con CV (profondità, C, learning rate…).  </li>
<li><strong>Loss/Errore:</strong> numero che misura “quanto sto sbagliando” durante l’addestramento.  </li>
<li><strong>Generalizzazione:</strong> andare bene su dati nuovi (test).  </li>
<li><strong>Calibrazione:</strong> rendere le <strong>probabilità</strong> coerenti con le frequenze reali.  </li>
<li><strong>OOF:</strong> predizioni su train ottenute senza vedere quel campione (da altri fold).  </li>
<li><strong>Youden:</strong> TPR−FPR; criterio di scelta soglia dalla ROC.  </li>
<li><strong>Ablation:</strong> test di robustezza rimuovendo feature importanti.</li>
</ul></div>
                <footer>
    <div class="footer-buttons">
        <div class="previous"><a href="../script/" title="Script"><span>Previous</span></a></div>
    </div>
    <div class="footer-note">
        <p>
            Built with <a href="http://www.mkdocs.org">MkDocs</a> using
            <a href="https://github.com/daizutabi/mkdocs-ivory">Ivory theme</a>.
        </p>
    </div>
</footer>
            </div>
        </main>
    </div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js"></script>
    <script src="../search/main.js"></script>
</body>

</html>